"""
GPUä¼˜åŒ–ç‰ˆæœ¬ - ç¾½æ¯›çƒäººä½“å…³é”®ç‚¹æ£€æµ‹å’Œéª¨æ¶ç»˜åˆ¶ (æœ€ç»ˆç¨³å®šç‰ˆ)
è§£å†³æ‰€æœ‰APIå…¼å®¹æ€§é—®é¢˜ï¼Œç¡®ä¿åœ¨å„ç§PyTorchç‰ˆæœ¬ä¸‹éƒ½èƒ½ç¨³å®šè¿è¡Œ

æ ¸å¿ƒç­–ç•¥ï¼š
1. ä½¿ç”¨ä¼ ç»Ÿç¨³å®šçš„API
2. å¢å¼ºé”™è¯¯å¤„ç†å’Œå®¹é”™æ€§
3. ä¿æŒåŸç‰ˆtest_person.pyçš„æˆåŠŸç®—æ³•
4. ç®€åŒ–æ··åˆç²¾åº¦å¤„ç†ï¼Œä¼˜å…ˆä¿è¯åŠŸèƒ½
"""

import cv2
import torch
from ultralytics import YOLO
import numpy as np
import os
import time
import gc
from typing import Optional, Tuple, List, Dict, Any

class GPUOptimizedDetector:
    """GPUä¼˜åŒ–çš„äººä½“æ£€æµ‹å™¨ - ç¨³å®šç‰ˆ"""
    
    def __init__(self, model_path: str = 'yolov8m-pose.pt', device: Optional[str] = None):
        """åˆå§‹åŒ–GPUä¼˜åŒ–æ£€æµ‹å™¨"""
        # è®¾å¤‡é…ç½®
        self.device = self._setup_device(device)
        self.use_amp = False  # æš‚æ—¶ç¦ç”¨æ··åˆç²¾åº¦ï¼Œç¡®ä¿ç¨³å®šæ€§
        
        print(f"ğŸš€ åˆå§‹åŒ–GPUä¼˜åŒ–æ£€æµ‹å™¨ (ç¨³å®šç‰ˆ)")
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"âš¡ æ··åˆç²¾åº¦: {self.use_amp} (ä¸ºç¡®ä¿ç¨³å®šæ€§æš‚æ—¶ç¦ç”¨)")
        
        # æ ¹æ®è®¾å¤‡é€‰æ‹©åˆé€‚çš„æ¨¡å‹
        if self.device.type == 'cuda':
            if 'n-pose' in model_path:
                model_path = model_path.replace('n-pose', 'm-pose')
            print(f"ğŸ¯ GPUæ¨¡å¼ï¼šä½¿ç”¨æ›´å¤§æ¨¡å‹ {model_path}")
        else:
            print(f"ğŸ¯ CPUæ¨¡å¼ï¼šä½¿ç”¨è½»é‡æ¨¡å‹ {model_path}")
        
        # åˆå§‹åŒ–YOLOæ¨¡å‹
        try:
            self.model = YOLO(model_path)
            if self.device.type == 'cuda':
                # ç®€åŒ–GPUè®¾ç½®ï¼Œé¿å…å¤æ‚API
                torch.backends.cudnn.benchmark = True
                self._simple_warmup()
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _setup_device(self, device: Optional[str]) -> torch.device:
        """è®¾ç½®è®¾å¤‡"""
        if device is None:
            if torch.cuda.is_available():
                device = 'cuda'
                gpu_name = torch.cuda.get_device_name(0)
                vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
                print(f"ğŸ¯ è‡ªåŠ¨æ£€æµ‹åˆ°GPU: {gpu_name} ({vram_gb:.1f}GB)")
            else:
                device = 'cpu'
                print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUæ¨¡å¼")
        
        device = torch.device(device)
        
        if device.type == 'cuda' and not torch.cuda.is_available():
            print("âŒ CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUæ¨¡å¼")
            device = torch.device('cpu')
        
        return device
    
    def _simple_warmup(self):
        """ç®€åŒ–çš„GPUé¢„çƒ­"""
        try:
            print("ğŸ”¥ GPUé¢„çƒ­ä¸­...")
            dummy_frame = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
            
            # ç®€å•é¢„çƒ­ï¼Œä¸ä½¿ç”¨æ··åˆç²¾åº¦
            with torch.no_grad():
                _ = self.model(dummy_frame, verbose=False)
            
            if self.device.type == 'cuda':
                torch.cuda.empty_cache()
            
            print("âœ… GPUé¢„çƒ­å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ GPUé¢„çƒ­å¤±è´¥: {e}")

def safe_float_robust(value):
    """
    æœ€å¼ºå¥çš„æ•°å€¼è½¬æ¢å‡½æ•°
    """
    try:
        # å¤„ç†å„ç§numpyç±»å‹
        if hasattr(value, 'item'):  # numpyæ ‡é‡
            return float(value.item())
        elif isinstance(value, np.ndarray):
            if value.size == 0:
                return 0.0
            elif value.size == 1:
                return float(value.flat[0])
            else:
                # å¤šå…ƒç´ æ•°ç»„ï¼Œè®°å½•è­¦å‘Šä½†ä¸å´©æºƒ
                print(f"    âš ï¸ å¤šå…ƒç´ æ•°ç»„è½¬æ ‡é‡: shape={value.shape}, å–é¦–å…ƒç´ ")
                return float(value.flat[0])
        elif isinstance(value, (int, float)):
            return float(value)
        else:
            # å…¶ä»–ç±»å‹å°è¯•ç›´æ¥è½¬æ¢
            return float(value)
    except Exception as e:
        print(f"    âŒ æ•°å€¼è½¬æ¢å¤±è´¥: {type(value)}, {e}, è¿”å›0.0")
        return 0.0

def select_nearest_person_robust(results, frame_height=640, frame_width=640):
    """
    æœ€ç¨³å¥çš„äººä½“é€‰æ‹©ç®—æ³•
    åŸºäºåŸç‰ˆtest_person.pyï¼Œå¢åŠ å…¨æ–¹ä½é”™è¯¯å¤„ç†
    """
    try:
        if not results or len(results) == 0:
            return None, None, 0

        result = results[0]

        if not hasattr(result, 'keypoints') or result.keypoints is None:
            return None, None, 0

        keypoints_data = result.keypoints

        if len(keypoints_data.xy) == 0:
            return None, None, 0

        best_idx = None
        max_score = 0.0
        person_scores = []
        max_possible_area = float(frame_height * frame_width)

        print(f"    ğŸ” å‘ç° {len(keypoints_data.xy)} ä¸ªäººç‰©å€™é€‰")

        for i in range(len(keypoints_data.xy)):
            try:
                # å®‰å…¨è·å–æ•°æ®
                coords = keypoints_data.xy[i].cpu().numpy().astype(np.float64)
                confidence = keypoints_data.conf[i].cpu().numpy().astype(np.float64)

                # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
                if coords.shape[0] == 0 or confidence.shape[0] == 0:
                    person_scores.append((i, 0.0, "empty_data"))
                    continue

                # è¿‡æ»¤ä½ç½®ä¿¡åº¦å…³é”®ç‚¹
                valid_mask = confidence > 0.2  # é™ä½é˜ˆå€¼
                valid_points = coords[valid_mask]
                valid_conf = confidence[valid_mask]

                if len(valid_points) < 2:  # æœ€ä½è¦æ±‚2ä¸ªå…³é”®ç‚¹
                    person_scores.append((i, 0.0, "insufficient_keypoints"))
                    continue

                # å®‰å…¨è®¡ç®—åŒ…å›´ç›’
                try:
                    min_vals = np.min(valid_points, axis=0)
                    max_vals = np.max(valid_points, axis=0)
                    
                    bbox_width = safe_float_robust(max_vals[0] - min_vals)
                    bbox_height = safe_float_robust(max_vals[1] - min_vals[1])
                    bbox_area = max(bbox_width * bbox_height, 1.0)  # é¿å…0é¢ç§¯
                    
                    # è®¡ç®—è´¨å¿ƒ
                    centroid_y = safe_float_robust(np.mean(valid_points[:, 1]))
                    position_score = max(0.0, min(1.0, centroid_y / frame_height))
                    
                    # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
                    avg_confidence = safe_float_robust(np.mean(valid_conf))
                    
                    # ç»¼åˆè¯„åˆ†ï¼ˆä¿æŒåŸç®—æ³•ï¼‰
                    area_weight = 0.5
                    position_weight = 0.3
                    confidence_weight = 0.2
                    
                    normalized_area = min(1.0, np.log10(bbox_area + 1) / np.log10(max_possible_area + 1))
                    normalized_position = position_score
                    normalized_confidence = avg_confidence
                    
                    composite_score = (
                        area_weight * normalized_area +
                        position_weight * normalized_position +
                        confidence_weight * normalized_confidence
                    )
                    
                    person_scores.append((
                        i,
                        composite_score,
                        f"area:{bbox_area:.0f}_pos:{centroid_y:.0f}_conf:{avg_confidence:.2f}_pts:{len(valid_points)}"
                    ))
                    
                    if composite_score > max_score:
                        max_score = composite_score
                        best_idx = i
                        
                except Exception as calc_e:
                    person_scores.append((i, 0.0, f"calc_error: {str(calc_e)[:30]}"))
                    continue
                    
            except Exception as person_e:
                person_scores.append((i, 0.0, f"person_error: {str(person_e)[:30]}"))
                continue

        # è°ƒè¯•è¾“å‡º
        print(f"    å€™é€‰äººè¯„åˆ†:")
        for idx, score, details in person_scores:
            marker = "ğŸ‘‘" if idx == best_idx else "  "
            print(f"    {marker} äººç‰©{idx}: å¾—åˆ†{score:.3f} ({details})")

        if best_idx is None:
            return None, None, 0.0

        # å®‰å…¨è¿”å›ç»“æœ
        try:
            best_keypoints = keypoints_data.xy[best_idx].cpu().numpy()
            best_confidence = keypoints_data.conf[best_idx].cpu().numpy()
            return best_keypoints, best_confidence, max_score
        except Exception as return_e:
            print(f"    âŒ è¿”å›ç»“æœæ—¶å‡ºé”™: {return_e}")
            return None, None, 0.0
            
    except Exception as main_e:
        print(f"  âŒ äººä½“é€‰æ‹©ä¸»å‡½æ•°å‡ºé”™: {main_e}")
        return None, None, 0.0

def preprocess_frame_simple(frame):
    """ç®€åŒ–çš„å›¾åƒé¢„å¤„ç†"""
    try:
        # æé«˜å¯¹æ¯”åº¦å’Œäº®åº¦
        frame = cv2.convertScaleAbs(frame, alpha=1.2, beta=10)
        
        # é”åŒ–æ»¤æ³¢
        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
        frame = cv2.filter2D(frame, -1, kernel)
        
        return frame
    except Exception as e:
        print(f"    âš ï¸ é¢„å¤„ç†å¤±è´¥: {e}, è¿”å›åŸå›¾")
        return frame

def extract_keypoints_and_draw_stable(video_path: str, 
                                     output_dir: str, 
                                     model_path: str = 'yolov8m-pose.pt',
                                     max_frames: int = 50,
                                     device: Optional[str] = None):
    """
    æœ€ç¨³å®šç‰ˆæœ¬çš„å…³é”®ç‚¹æå–å’Œç»˜åˆ¶
    """
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = GPUOptimizedDetector(model_path, device)
    
    try:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
            return
    except Exception as e:
        print(f"âŒ è§†é¢‘åŠ è½½å¤±è´¥: {e}")
        return

    os.makedirs(output_dir, exist_ok=True)

    # è·å–è§†é¢‘ä¿¡æ¯
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    print(f"ğŸ¬ å¤„ç†è§†é¢‘: {os.path.basename(video_path)}")
    print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {width}x{height}, {total_frames}å¸§, {fps:.1f}fps")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ¯ æœ€å¤§å¤„ç†å¸§æ•°: {max_frames}")
    print("-" * 50)

    frame_idx = 0
    saved_idx = 0
    detection_stats = []
    
    total_start_time = time.time()

    # éª¨æ¶è¿æ¥å®šä¹‰
    skeleton_pairs = [
        (0, 1), (0, 2), (1, 3), (2, 4),        # å¤´éƒ¨
        (5, 6), (5, 11), (6, 12), (11, 12),    # èº¯å¹²
        (5, 7), (7, 9),                        # å·¦è‡‚
        (6, 8), (8, 10),                       # å³è‡‚
        (11, 13), (13, 15),                    # å·¦è…¿
        (12, 14), (14, 16)                     # å³è…¿
    ]

    while frame_idx < max_frames and frame_idx < total_frames:
        ret, frame = cap.read()
        if not ret:
            break

        print(f"\nğŸ” å¤„ç†å¸§ {frame_idx + 1}/{min(max_frames, total_frames)}")

        try:
            # ä¿å®ˆçš„æ™ºèƒ½è£å‰ª
            original_height, original_width = frame.shape[:2]
            crop_y1 = max(0, int(original_height * 0.02))  # éå¸¸ä¿å®ˆçš„è£å‰ª
            crop_y2 = min(original_height, int(original_height * 0.98))
            crop_x1 = max(0, int(original_width * 0.02))
            crop_x2 = min(original_width, int(original_width * 0.98))
            
            cropped_frame = frame[crop_y1:crop_y2, crop_x1:crop_x2]

            # é¢„å¤„ç†
            processed_frame = preprocess_frame_simple(cropped_frame)

            # è°ƒæ•´å°ºå¯¸
            target_size = 640
            frame_resized = cv2.resize(processed_frame, (target_size, target_size))

            # ç¨³å®šçš„YOLOæ£€æµ‹
            try:
                with torch.no_grad():
                    results = detector.model(
                        frame_resized, 
                        verbose=False, 
                        conf=0.03  # è¿›ä¸€æ­¥é™ä½é˜ˆå€¼
                    )
            except Exception as detection_e:
                print(f"  âŒ YOLOæ£€æµ‹å¤±è´¥: {detection_e}")
                continue

            # é€‰æ‹©æœ€ä½³äººä½“
            best_keypoints, best_confidence, best_score = select_nearest_person_robust(
                results, target_size, target_size
            )

            if best_keypoints is not None and best_confidence is not None and best_score > 0:
                print(f"  âœ… æ£€æµ‹æˆåŠŸï¼Œæœ€ä½³å€™é€‰å¾—åˆ†: {best_score:.3f}")

                # åˆ›å»ºç»˜åˆ¶å›¾åƒ
                draw_img = frame_resized.copy()

                # å¤„ç†å…³é”®ç‚¹
                valid_keypoints = []
                valid_confidences = []

                for i in range(min(len(best_keypoints), 17)):  # ç¡®ä¿ä¸è¶…è¿‡17ä¸ªå…³é”®ç‚¹
                    try:
                        x = safe_float_robust(best_keypoints[i][0])
                        y = safe_float_robust(best_keypoints[i][1])
                        conf = safe_float_robust(best_confidence[i]) if i < len(best_confidence) else 0.0

                        # æ›´å®½æ¾çš„æ¡ä»¶
                        if conf > 0.1 and 0 <= x < target_size and 0 <= y < target_size:
                            valid_keypoints.append((i, x, y, conf))
                            valid_confidences.append(conf)
                    except Exception as kp_e:
                        continue

                print(f"    ğŸ“ æœ‰æ•ˆå…³é”®ç‚¹: {len(valid_keypoints)}/17")

                if len(valid_keypoints) >= 2:  # è‡³å°‘2ä¸ªå…³é”®ç‚¹æ‰ç»˜åˆ¶
                    # ç»˜åˆ¶å…³é”®ç‚¹
                    for i, x, y, conf in valid_keypoints:
                        if conf > 0.6:
                            color = (0, 255, 0)    # ç»¿è‰²
                        elif conf > 0.3:
                            color = (0, 255, 255)  # é»„è‰²
                        else:
                            color = (0, 0, 255)    # çº¢è‰²

                        cv2.circle(draw_img, (int(x), int(y)), 4, color, -1)
                        cv2.putText(draw_img, str(i), (int(x)-5, int(y)-5),
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255,255,255), 1)

                    # ç»˜åˆ¶éª¨æ¶
                    kp_dict = {i: (x, y, conf) for i, x, y, conf in valid_keypoints}

                    for (p1, p2) in skeleton_pairs:
                        if p1 in kp_dict and p2 in kp_dict:
                            x1, y1, conf1 = kp_dict[p1]
                            x2, y2, conf2 = kp_dict[p2]

                            if conf1 > 0.15 and conf2 > 0.15:
                                cv2.line(draw_img, (int(x1), int(y1)), (int(x2), int(y2)),
                                        (0, 0, 255), 2)

                    # æ·»åŠ ä¿¡æ¯
                    avg_conf = np.mean(valid_confidences) if valid_confidences else 0

                    info_text = [
                        f"Frame: {frame_idx + 1}",
                        f"Score: {best_score:.3f}",
                        f"Conf: {avg_conf:.2f}",
                        f"Points: {len(valid_keypoints)}/17"
                    ]

                    for i, text in enumerate(info_text):
                        cv2.putText(draw_img, text, (10, 25 + i * 20),
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

                    # ä¿å­˜å›¾ç‰‡
                    output_filename = f"frame_{saved_idx:04d}_score_{best_score:.3f}.jpg"
                    output_path = os.path.join(output_dir, output_filename)

                    if cv2.imwrite(output_path, draw_img):
                        print(f"  ğŸ’¾ å·²ä¿å­˜: {output_filename}")
                        saved_idx += 1

                        detection_stats.append({
                            'frame': frame_idx,
                            'score': float(best_score),
                            'confidence': float(avg_conf),
                            'valid_points': len(valid_keypoints)
                        })
                    else:
                        print(f"  âŒ ä¿å­˜å¤±è´¥: {output_filename}")
                else:
                    print(f"  âš ï¸ æœ‰æ•ˆå…³é”®ç‚¹ä¸è¶³ï¼Œè·³è¿‡ç»˜åˆ¶")
            else:
                print(f"  âŒ æœªæ£€æµ‹åˆ°æœ‰æ•ˆäººä½“")

        except Exception as frame_e:
            print(f"  âš ï¸ å¤„ç†å¸§ {frame_idx + 1} æ—¶å‡ºé”™: {frame_e}")

        frame_idx += 1

        # GPUå†…å­˜ç®¡ç†
        if detector.device.type == 'cuda' and frame_idx % 5 == 0:
            torch.cuda.empty_cache()

    cap.release()

    # æœ€ç»ˆç»Ÿè®¡
    total_time = time.time() - total_start_time
    
    print("\n" + "=" * 50)
    print("ğŸ‰ å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š æ€»å¤„ç†å¸§æ•°: {frame_idx}")
    print(f"âœ… æˆåŠŸæ£€æµ‹å¸§æ•°: {saved_idx}")
    
    if frame_idx > 0:
        success_rate = saved_idx/frame_idx*100
        print(f"ğŸ“ˆ æ£€æµ‹æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"â±ï¸  æ€»å¤„ç†æ—¶é—´: {total_time:.1f}ç§’")
        print(f"ğŸš€ å¹³å‡FPS: {frame_idx/total_time:.1f}")
    
    print(f"ğŸ“ è¾“å‡ºå›¾ç‰‡ä¿å­˜åˆ°: {output_dir}")

    if detection_stats:
        avg_score = sum(s['score'] for s in detection_stats) / len(detection_stats)
        avg_conf = sum(s['confidence'] for s in detection_stats) / len(detection_stats)
        avg_points = sum(s['valid_points'] for s in detection_stats) / len(detection_stats)

        print(f"ğŸ“Š å¹³å‡è¯„åˆ†: {avg_score:.3f}")
        print(f"ğŸ“Š å¹³å‡ç½®ä¿¡åº¦: {avg_conf:.3f}")
        print(f"ğŸ“Š å¹³å‡æœ‰æ•ˆå…³é”®ç‚¹: {avg_points:.1f}/17")
    
    # æˆåŠŸç‡è¯„ä»·
    if saved_idx > 0:
        success_rate = saved_idx/frame_idx*100
        if success_rate >= 50:
            print(f"ğŸ† æ£€æµ‹æˆåŠŸç‡å¾ˆé«˜: {success_rate:.1f}%")
        elif success_rate >= 20:
            print(f"ğŸ‘ æ£€æµ‹æˆåŠŸç‡è‰¯å¥½: {success_rate:.1f}%")
        else:
            print(f"ğŸ“ˆ æ£€æµ‹æˆåŠŸç‡è¾ƒä½: {success_rate:.1f}%ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´å‚æ•°")

    # æ¸…ç†
    if detector.device.type == 'cuda':
        torch.cuda.empty_cache()
        gc.collect()

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="GPUä¼˜åŒ–çš„ç¾½æ¯›çƒäººä½“å…³é”®ç‚¹æ£€æµ‹ (æœ€ç¨³å®šç‰ˆ)")
    parser.add_argument("--video", default="data/split/train/14_Smash/2022-08-30_19-10-16_dataset_set1_130_011066_011084_A_14.mp4",
                       help="è¾“å…¥è§†é¢‘è·¯å¾„")
    parser.add_argument("--output", default="debug_output_stable",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--model", default="yolov8m-pose.pt",
                       help="YOLOæ¨¡å‹è·¯å¾„")
    parser.add_argument("--max-frames", type=int, default=50,
                       help="æœ€å¤§å¤„ç†å¸§æ•°")
    parser.add_argument("--device", choices=['auto', 'cuda', 'cpu'], default='auto',
                       help="æŒ‡å®šè®¾å¤‡")
    parser.add_argument("--cpu", action="store_true",
                       help="å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼")
    
    args = parser.parse_args()
    
    # è®¾å¤‡é€‰æ‹©
    if args.cpu:
        device = 'cpu'
    elif args.device == 'auto':
        device = None
    else:
        device = args.device
    
    # æ£€æŸ¥è§†é¢‘æ–‡ä»¶
    if os.path.exists(args.video):
        extract_keypoints_and_draw_stable(
            video_path=args.video,
            output_dir=args.output,
            model_path=args.model,
            max_frames=args.max_frames,
            device=device
        )
    else:
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {args.video}")
        print("è¯·ç¡®è®¤è§†é¢‘è·¯å¾„æ­£ç¡®ï¼Œæˆ–ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŒ‡å®šè§†é¢‘ï¼š")
        print(f"python {__file__} --video /path/to/your/video.mp4")

if __name__ == "__main__":
    main()
