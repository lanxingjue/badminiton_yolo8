import cv2
from ultralytics import YOLO
import numpy as np

def debug_single_video(video_path):
    """è°ƒè¯•å•ä¸ªè§†é¢‘çš„æ£€æµ‹æƒ…å†µ - ä¿®æ­£ç‰ˆ"""
    model = YOLO('yolov8n-pose.pt')
    cap = cv2.VideoCapture(video_path)
    
    print(f"ðŸ” è°ƒè¯•è§†é¢‘: {video_path}")
    
    if not cap.isOpened():
        print("âŒ æ— æ³•æ‰“å¼€è§†é¢‘")
        return
    
    # èŽ·å–è§†é¢‘ä¿¡æ¯
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    print(f"ðŸ“Š è§†é¢‘ä¿¡æ¯: {width}x{height}, {total_frames}å¸§, {fps}fps")
    
    # æµ‹è¯•å¤šç§æ£€æµ‹ç­–ç•¥
    strategies = [
        ("åŽŸå§‹å¸§", lambda f: f),
        ("è£å‰ªå¸§", lambda f: f[int(height*0.1):int(height*0.9), int(width*0.1):int(width*0.9)]),
        ("æ”¾å¤§è£å‰ª", lambda f: cv2.resize(f[int(height*0.2):int(height*0.8), int(width*0.2):int(width*0.8)], (640, 640)))
    ]
    
    for strategy_name, preprocess in strategies:
        print(f"\nðŸ§ª æµ‹è¯•ç­–ç•¥: {strategy_name}")
        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # é‡ç½®åˆ°å¼€å§‹
        
        detections = 0
        test_frames = min(20, total_frames)
        
        for i in range(test_frames):
            ret, frame = cap.read()
            if not ret:
                break
                
            try:
                processed_frame = preprocess(frame)
                if processed_frame.size == 0:
                    continue
                
                # ðŸ”§ ä¿®æ­£ï¼šé€‚é…æ–°ç‰ˆultralytics API
                results = model(processed_frame, verbose=False, conf=0.01)
                
                # ðŸ”§ å…³é”®ä¿®æ­£ï¼šresultsæ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                if results and len(results) > 0:
                    result = results[0]  # å–ç¬¬ä¸€ä¸ªç»“æžœ
                    
                    if hasattr(result, 'keypoints') and result.keypoints is not None:
                        keypoints_data = result.keypoints
                        
                        if len(keypoints_data.xy) > 0:
                            detections += 1
                            print(f"  âœ… å¸§ {i}: æ£€æµ‹åˆ° {len(keypoints_data.xy)} ä¸ªäººä½“")
                            
                            # æ˜¾ç¤ºæ£€æµ‹è¯¦æƒ…
                            for j, kp_tensor in enumerate(keypoints_data.xy):
                                if j < len(keypoints_data.conf):
                                    conf = keypoints_data.conf[j].cpu().numpy()
                                    print(f"    äººç‰©{j}: å¹³å‡ç½®ä¿¡åº¦ {conf.mean():.3f}")
                        else:
                            print(f"  âŒ å¸§ {i}: æ£€æµ‹åˆ°keypointsä½†æ— åæ ‡æ•°æ®")
                    else:
                        print(f"  âŒ å¸§ {i}: æ— keypointsæ£€æµ‹")
                else:
                    print(f"  âŒ å¸§ {i}: æ— ç»“æžœè¿”å›ž")
                    
            except Exception as e:
                print(f"  âš ï¸ å¸§ {i}: å¤„ç†å‡ºé”™ {e}")
        
        success_rate = detections / test_frames * 100
        print(f"ðŸ“ˆ {strategy_name} æˆåŠŸçŽ‡: {success_rate:.1f}% ({detections}/{test_frames})")
    
    cap.release()

# æµ‹è¯•ä¸€ä¸ªå…·ä½“çš„å¤±è´¥è§†é¢‘
debug_single_video("data/split/train/14_Smash/2022-08-30_19-10-16_dataset_set1_130_011066_011084_A_14.mp4")
