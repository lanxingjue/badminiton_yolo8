"""
åŠ¨ä½œæ£€æµ‹å™¨ - è´Ÿè´£ä»è§†é¢‘ä¸­æå–å…³é”®ç‚¹å’Œåˆ†ç±»åŠ¨ä½œ
LinusåŸåˆ™ï¼šä¸€ä¸ªæ–‡ä»¶åšä¸€ä»¶äº‹ï¼Œåšå¥½å®ƒ
é›†æˆGPUä¼˜åŒ–å’ŒæˆåŠŸçš„äººä½“é€‰æ‹©ç®—æ³•
"""

import cv2
import torch
import torch.cuda.amp as amp
import numpy as np
from ultralytics import YOLO
from typing import List, Optional
import warnings

from core import Keypoints, BadmintonShot
from config import MODEL_CONFIG, RAW_CLASSES, TRAINING_CONFIG

# å¿½ç•¥YOLOçš„è­¦å‘Šä¿¡æ¯
warnings.filterwarnings("ignore", category=UserWarning)

def safe_float(value):
    """
    å®‰å…¨çš„æ•°å€¼è½¬æ¢ï¼Œå¤„ç†æ‰€æœ‰numpyç±»å‹
    ä»trainer.pyé›†æˆçš„å‡½æ•°
    """
    if isinstance(value, (np.ndarray, np.generic)):
        if hasattr(value, 'item'):
            return float(value.item())
        elif len(value.shape) == 0:  # 0ç»´æ•°ç»„ï¼ˆæ ‡é‡ï¼‰
            return float(value)
        elif value.size == 1:  # åªæœ‰ä¸€ä¸ªå…ƒç´ 
            return float(value.flat[0])
        else:
            raise ValueError(f"Cannot convert array of size {value.size} to scalar")
    else:
        return float(value)

def select_nearest_person_keypoints(results, frame_height=640, frame_width=640):
    """
    ä»YOLOv8å§¿æ€æ£€æµ‹ç»“æœä¸­é€‰æ‹©æœ€é è¿‘æ‘„åƒå¤´çš„äºº
    ä»trainer.pyé›†æˆçš„æˆåŠŸç®—æ³•
    """
    if not results or len(results) == 0:
        return None, None
    
    result = results[0]
    
    if not hasattr(result, 'keypoints') or result.keypoints is None:
        return None, None
    
    keypoints_data = result.keypoints
    
    if len(keypoints_data.xy) == 0:
        return None, None
    
    best_idx = None
    max_score = 0.0
    max_possible_area = float(frame_height * frame_width)
    
    for i in range(len(keypoints_data.xy)):
        try:
            coords = keypoints_data.xy[i].cpu().numpy().astype(np.float64)
            confidence = keypoints_data.conf[i].cpu().numpy().astype(np.float64)
            
            # è¿‡æ»¤ä½ç½®ä¿¡åº¦å…³é”®ç‚¹
            valid_mask = confidence > 0.3
            valid_points = coords[valid_mask]
            valid_conf = confidence[valid_mask]
            
            if len(valid_points) < 5:  # è‡³å°‘éœ€è¦5ä¸ªé«˜ç½®ä¿¡åº¦å…³é”®ç‚¹
                continue
            
            # è®¡ç®—åŒ…å›´ç›’é¢ç§¯
            min_xy = valid_points.min(axis=0)
            max_xy = valid_points.max(axis=0)
            
            bbox_width = safe_float(max_xy[0] - min_xy[0])
            bbox_height = safe_float(max_xy[1] - min_xy[1])
            bbox_area = bbox_width * bbox_height
            
            # è®¡ç®—è´¨å¿ƒä½ç½®
            centroid_y = safe_float(valid_points[:, 1].mean())
            position_score = centroid_y / frame_height
            
            # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
            avg_confidence = safe_float(valid_conf.mean())
            
            # ç»¼åˆè¯„åˆ†ï¼šé¢ç§¯50% + ä½ç½®30% + ç½®ä¿¡åº¦20%
            area_weight = 0.5
            position_weight = 0.3
            confidence_weight = 0.2
            
            # é¢ç§¯å½’ä¸€åŒ–
            if bbox_area > 0:
                normalized_area = min(np.log10(bbox_area + 1) / np.log10(max_possible_area + 1), 1.0)
            else:
                normalized_area = 0.0
            
            normalized_position = min(position_score, 1.0)
            normalized_confidence = min(avg_confidence, 1.0)
            
            composite_score = (
                area_weight * normalized_area +
                position_weight * normalized_position +
                confidence_weight * normalized_confidence
            )
            
            if composite_score > max_score:
                max_score = composite_score
                best_idx = i
                
        except Exception as e:
            continue
    
    if best_idx is None:
        return None, None
    
    # è¿”å›æœ€ä½³å€™é€‰äººçš„å…³é”®ç‚¹å’Œç½®ä¿¡åº¦
    best_keypoints = keypoints_data.xy[best_idx].cpu().numpy()
    best_confidence = keypoints_data.conf[best_idx].cpu().numpy()
    
    return best_keypoints, best_confidence

class BadmintonDetector:
    """
    ç¾½æ¯›çƒåŠ¨ä½œæ£€æµ‹å™¨ - GPUä¼˜åŒ–ç‰ˆæœ¬
    
    èŒè´£ï¼š
    1. ä»è§†é¢‘å¸§ä¸­æå–äººä½“å…³é”®ç‚¹
    2. å¯¹å…³é”®ç‚¹åºåˆ—è¿›è¡ŒåŠ¨ä½œåˆ†ç±»
    3. ç”ŸæˆåŠ¨ä½œè´¨é‡åˆ†æç»“æœ
    """
    
    def __init__(self, pose_model_path: str = 'yolov8n-pose.pt',
                 action_model_path: Optional[str] = None):
        """
        åˆå§‹åŒ–æ£€æµ‹å™¨
        
        Args:
            pose_model_path: YOLOv8å§¿æ€æ£€æµ‹æ¨¡å‹è·¯å¾„
            action_model_path: è®­ç»ƒå¥½çš„åŠ¨ä½œåˆ†ç±»æ¨¡å‹è·¯å¾„
        """
        print("åˆå§‹åŒ–ç¾½æ¯›çƒåŠ¨ä½œæ£€æµ‹å™¨...")
        
        # ğŸ”§ GPUä¼˜åŒ–ï¼šæ ¹æ®ç¡¬ä»¶é€‰æ‹©åˆé€‚çš„å§¿æ€æ£€æµ‹æ¨¡å‹
        if torch.cuda.is_available():
            # GPUæ¨¡å¼ä½¿ç”¨æ›´å¤§æ›´ç²¾ç¡®çš„æ¨¡å‹
            if pose_model_path == 'yolov8n-pose.pt':
                pose_model_path = 'yolov8m-pose.pt'
            print(f"ğŸ¯ GPUæ¨¡å¼ï¼šä½¿ç”¨æ›´å¤§çš„å§¿æ€æ£€æµ‹æ¨¡å‹ {pose_model_path}")
        else:
            print(f"ğŸ¯ CPUæ¨¡å¼ï¼šä½¿ç”¨è½»é‡çº§å§¿æ€æ£€æµ‹æ¨¡å‹ {pose_model_path}")
        
        # åˆå§‹åŒ–å§¿æ€æ£€æµ‹æ¨¡å‹
        try:
            self.pose_model = YOLO(pose_model_path)
            print(f"âœ… å§¿æ€æ£€æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ: {pose_model_path}")
        except Exception as e:
            print(f"âŒ å§¿æ€æ£€æµ‹æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
        
        # æ„å»ºåŠ¨ä½œåˆ†ç±»å™¨ç½‘ç»œ
        self.action_classifier = self._build_action_classifier()
        
        # è®¾ç½®è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.action_classifier.to(self.device)
        
        # ğŸ”§ æ··åˆç²¾åº¦æ”¯æŒ
        self.use_amp = torch.cuda.is_available()
        if self.use_amp:
            print("âš¡ å¯ç”¨æ··åˆç²¾åº¦æ¨ç†åŠ é€Ÿ")
        
        # å¦‚æœæä¾›äº†è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ŒåŠ è½½æƒé‡
        if action_model_path and os.path.exists(action_model_path):
            try:
                self.action_classifier.load_state_dict(torch.load(action_model_path, map_location=self.device))
                self.action_classifier.eval()
                print(f"âœ… åŠ¨ä½œåˆ†ç±»æ¨¡å‹åŠ è½½æˆåŠŸ: {action_model_path}")
            except Exception as e:
                print(f"âš ï¸ åŠ¨ä½œåˆ†ç±»æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨æœªè®­ç»ƒæ¨¡å‹: {e}")
        else:
            print("âš ï¸ æœªæä¾›åŠ¨ä½œåˆ†ç±»æ¨¡å‹ï¼Œä½¿ç”¨æœªè®­ç»ƒæ¨¡å‹")
        
        # åˆå§‹åŒ–å¸§ç¼“å†²åŒº
        self.frame_buffer: List[Keypoints] = []
        self.buffer_size = MODEL_CONFIG['sequence_length']
        
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"ğŸ”§ åºåˆ—ç¼“å†²åŒºå¤§å°: {self.buffer_size}")
    
    def _build_action_classifier(self) -> torch.nn.Module:
        """
        æ„å»ºåŠ¨ä½œåˆ†ç±»å™¨ç½‘ç»œ
        LinusåŸåˆ™ï¼šç®€å•èƒœè¿‡å¤æ‚ï¼Œå¯ç†è§£èƒœè¿‡èªæ˜
        """
        input_dim = MODEL_CONFIG['keypoints'] * 2 * MODEL_CONFIG['sequence_length']  # 17*2*10=340
        
        return torch.nn.Sequential(
            # è¾“å…¥å±‚ï¼š340ç»´ -> 256ç»´
            torch.nn.Linear(input_dim, 256),
            torch.nn.BatchNorm1d(256),
            torch.nn.ReLU(inplace=True),  # ğŸ”§ ä½¿ç”¨inplaceæ“ä½œèŠ‚çœå†…å­˜
            torch.nn.Dropout(MODEL_CONFIG['dropout_rate']),
            
            # éšè—å±‚ï¼š256ç»´ -> 128ç»´
            torch.nn.Linear(256, 128),
            torch.nn.BatchNorm1d(128),
            torch.nn.ReLU(inplace=True),
            torch.nn.Dropout(MODEL_CONFIG['dropout_rate']),
            
            # è¾“å‡ºå±‚ï¼š128ç»´ -> 18ç±»
            torch.nn.Linear(128, 18)
        )
    
    def extract_keypoints(self, frame: np.ndarray) -> Optional[Keypoints]:
        """
        ä»å•å¸§å›¾åƒä¸­æå–äººä½“å…³é”®ç‚¹
        é›†æˆä¼˜åŒ–çš„äººä½“é€‰æ‹©ç®—æ³•
        """
        try:
            # YOLOv8å§¿æ€æ£€æµ‹
            results = self.pose_model(frame, verbose=False)
            
            # ğŸ¯ ä½¿ç”¨ç›¸åŒçš„äººä½“é€‰æ‹©ç®—æ³•
            best_keypoints, best_confidence = select_nearest_person_keypoints(
                results, frame.shape[0], frame.shape[1]
            )
            
            if best_keypoints is None or best_confidence is None:
                return None
            
            # æ£€æŸ¥ç½®ä¿¡åº¦æ˜¯å¦è¶³å¤Ÿé«˜
            avg_confidence = safe_float(best_confidence.mean())
            if avg_confidence < TRAINING_CONFIG['min_confidence_threshold']:
                return None
            
            return Keypoints(points=best_keypoints, confidence=best_confidence)
            
        except Exception as e:
            print(f"å…³é”®ç‚¹æå–å¤±è´¥: {e}")
            return None
    
    def process_frame(self, frame: np.ndarray) -> Optional[BadmintonShot]:
        """
        å¤„ç†å•å¸§å›¾åƒï¼Œè¿”å›åŠ¨ä½œåˆ†æç»“æœï¼ˆå½“ç¼“å†²åŒºæ»¡æ—¶ï¼‰
        
        Args:
            frame: è¾“å…¥å›¾åƒå¸§
            
        Returns:
            BadmintonShotå¯¹è±¡æˆ–None
        """
        # è°ƒæ•´å¸§å¤§å°åˆ°æ¨¡å‹è¾“å…¥å°ºå¯¸
        resized_frame = cv2.resize(frame, MODEL_CONFIG['input_size'])
        
        # æå–å…³é”®ç‚¹
        keypoints = self.extract_keypoints(resized_frame)
        
        if keypoints is None:
            return None
        
        # æ›´æ–°åºåˆ—ç¼“å†²åŒº
        self.frame_buffer.append(keypoints)
        if len(self.frame_buffer) > self.buffer_size:
            self.frame_buffer.pop(0)  # ç§»é™¤æœ€æ—§çš„å¸§
        
        # å½“ç¼“å†²åŒºæ»¡æ—¶è¿›è¡ŒåŠ¨ä½œåˆ†ç±»
        if len(self.frame_buffer) == self.buffer_size:
            return self._classify_action()
        
        return None
    
    def _classify_action(self) -> BadmintonShot:
        """
        å¯¹ç¼“å†²åŒºä¸­çš„å…³é”®ç‚¹åºåˆ—è¿›è¡ŒåŠ¨ä½œåˆ†ç±»
        GPUä¼˜åŒ–ç‰ˆæœ¬
        
        Returns:
            BadmintonShotåˆ†æç»“æœ
        """
        try:
            # å‡†å¤‡è¾“å…¥æ•°æ®ï¼šå°†å…³é”®ç‚¹åºåˆ—å±•å¹³ä¸ºä¸€ç»´å‘é‡
            sequence_data = []
            for keypoints in self.frame_buffer:
                sequence_data.extend(keypoints.points.flatten())
            
            # è½¬æ¢ä¸ºPyTorchå¼ é‡
            input_tensor = torch.FloatTensor(sequence_data).unsqueeze(0).to(self.device, non_blocking=True)
            
            # å‰å‘æ¨ç†
            with torch.no_grad():
                if self.use_amp:
                    # ğŸ”§ ä½¿ç”¨æ··åˆç²¾åº¦æ¨ç†
                    with amp.autocast():
                        outputs = self.action_classifier(input_tensor)
                else:
                    outputs = self.action_classifier(input_tensor)
                
                probabilities = torch.softmax(outputs, dim=1)
                predicted_class = torch.argmax(probabilities, dim=1).item()
                confidence = probabilities.max().item()
            
            # åˆ›å»ºåˆ†æç»“æœ
            result = BadmintonShot.from_raw_class(
                raw_class=predicted_class,
                keypoints_seq=self.frame_buffer.copy(),
                classification_confidence=confidence
            )
            
            return result
            
        except Exception as e:
            print(f"åŠ¨ä½œåˆ†ç±»å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤ç»“æœ
            return BadmintonShot.from_raw_class(
                raw_class=0,  # é»˜è®¤ä¸ºçŸ­å‘çƒ
                keypoints_seq=self.frame_buffer.copy(),
                classification_confidence=0.0
            )
    
    def process_video(self, video_path: str) -> List[BadmintonShot]:
        """
        å¤„ç†æ•´ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œè¿”å›æ‰€æœ‰æ£€æµ‹åˆ°çš„åŠ¨ä½œ
        GPUä¼˜åŒ–ç‰ˆæœ¬
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            BadmintonShotç»“æœåˆ—è¡¨
        """
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            return []
        
        results = []
        frame_count = 0
        max_frames = TRAINING_CONFIG['max_frames_per_video'] * 2  # GPUå¯ä»¥å¤„ç†æ›´å¤šå¸§
        
        print(f"å¼€å§‹å¤„ç†è§†é¢‘: {video_path}")
        
        while frame_count < max_frames:
            ret, frame = cap.read()
            if not ret:
                break
            
            result = self.process_frame(frame)
            if result is not None:
                results.append(result)
                print(f"æ£€æµ‹åˆ°åŠ¨ä½œ: {result.category_name} (ç½®ä¿¡åº¦: {result.confidence:.2f})")
            
            frame_count += 1
            
            # æ˜¾ç¤ºè¿›åº¦
            if frame_count % 100 == 0:
                print(f"å·²å¤„ç† {frame_count} å¸§...")
        
        cap.release()
        print(f"è§†é¢‘å¤„ç†å®Œæˆï¼Œå…±æ£€æµ‹åˆ° {len(results)} ä¸ªåŠ¨ä½œ")
        return results
    
    def reset_buffer(self):
        """é‡ç½®å¸§ç¼“å†²åŒº"""
        self.frame_buffer.clear()
    
    def get_buffer_status(self) -> dict:
        """è·å–ç¼“å†²åŒºçŠ¶æ€ä¿¡æ¯"""
        return {
            'current_size': len(self.frame_buffer),
            'max_size': self.buffer_size,
            'is_ready': len(self.frame_buffer) == self.buffer_size
        }

# å¯¼å…¥osæ¨¡å—
import os
