"""
æ¨¡å‹è®­ç»ƒå™¨ - è®­ç»ƒç¾½æ¯›çƒåŠ¨ä½œåˆ†ç±»æ¨¡å‹
LinusåŸåˆ™ï¼šå·¥å…·è¦ç®€å•ã€å¯é ã€å¯é¢„æµ‹
é›†æˆäº†æˆåŠŸçš„äººä½“é€‰æ‹©ç®—æ³•
"""

import os
import glob
import cv2
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader
from typing import List, Tuple, Optional
import random
import time
import json
from datetime import datetime
from ultralytics import YOLO

from detector import BadmintonDetector
from core import Keypoints
from config import MODEL_CONFIG, TRAINING_CONFIG, RAW_CLASSES

def preprocess_frame(frame):
    """
    æé«˜å›¾åƒè´¨é‡ï¼Œå¢å¼ºäººä½“æ£€æµ‹æ•ˆæœ
    """
    # æé«˜å¯¹æ¯”åº¦å’Œäº®åº¦
    frame = cv2.convertScaleAbs(frame, alpha=1.2, beta=10)
    # é”åŒ–æ»¤æ³¢å»é™¤è¿åŠ¨æ¨¡ç³Š
    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
    frame = cv2.filter2D(frame, -1, kernel)
    return frame

def safe_float(value):
    """
    å®‰å…¨çš„æ•°å€¼è½¬æ¢ï¼Œå¤„ç†æ‰€æœ‰numpyç±»å‹
    é›†æˆè‡ªtest_person.pyçš„æˆåŠŸå®ç°
    """
    if isinstance(value, (np.ndarray, np.generic)):
        if hasattr(value, 'item'):
            return float(value.item())
        elif len(value.shape) == 0:  # 0ç»´æ•°ç»„ï¼ˆæ ‡é‡ï¼‰
            return float(value)
        elif value.size == 1:  # åªæœ‰ä¸€ä¸ªå…ƒç´ 
            return float(value.flat[0])
        else:
            raise ValueError(f"Cannot convert array of size {value.size} to scalar")
    else:
        return float(value)

def select_nearest_person_keypoints(results, frame_height=640, frame_width=640):
    """
    ä»YOLOv8å§¿æ€æ£€æµ‹ç»“æœä¸­é€‰æ‹©æœ€é è¿‘æ‘„åƒå¤´çš„äºº
    é›†æˆè‡ªtest_person.pyçš„æˆåŠŸå®ç°ï¼Œä¸“é—¨é’ˆå¯¹è®­ç»ƒæ•°æ®ä¼˜åŒ–
    
    åˆ¤æ–­æ ‡å‡†ï¼š
    1. å…³é”®ç‚¹åŒ…å›´ç›’é¢ç§¯æœ€å¤§ï¼ˆäººä½“åœ¨ç”»é¢ä¸­æœ€å¤§ï¼‰
    2. å…³é”®ç‚¹è´¨å¿ƒä½ç½®æœ€é è¿‘ç”»é¢åº•éƒ¨ï¼ˆæ›´é è¿‘æ‘„åƒå¤´ï¼‰
    3. ç»¼åˆè¯„åˆ†é€‰æ‹©æœ€ä½³å€™é€‰
    """
    if not results or len(results) == 0:
        return None, None
    
    result = results[0]
    
    if not hasattr(result, 'keypoints') or result.keypoints is None:
        return None, None
    
    keypoints_data = result.keypoints
    
    if len(keypoints_data.xy) == 0:
        return None, None
    
    best_idx = None
    max_score = 0.0
    max_possible_area = float(frame_height * frame_width)
    
    for i in range(len(keypoints_data.xy)):
        try:
            coords = keypoints_data.xy[i].cpu().numpy().astype(np.float64)
            confidence = keypoints_data.conf[i].cpu().numpy().astype(np.float64)
            
            # è¿‡æ»¤ä½ç½®ä¿¡åº¦å…³é”®ç‚¹
            valid_mask = confidence > 0.3
            valid_points = coords[valid_mask]
            valid_conf = confidence[valid_mask]
            
            if len(valid_points) < 5:  # è‡³å°‘éœ€è¦5ä¸ªé«˜ç½®ä¿¡åº¦å…³é”®ç‚¹
                continue
            
            # è®¡ç®—åŒ…å›´ç›’é¢ç§¯
            min_xy = valid_points.min(axis=0)
            max_xy = valid_points.max(axis=0)
            
            bbox_width = safe_float(max_xy[0] - min_xy[0])
            bbox_height = safe_float(max_xy[1] - min_xy[1])
            bbox_area = bbox_width * bbox_height
            
            # è®¡ç®—è´¨å¿ƒä½ç½®
            centroid_y = safe_float(valid_points[:, 1].mean())
            position_score = centroid_y / frame_height
            
            # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
            avg_confidence = safe_float(valid_conf.mean())
            
            # ç»¼åˆè¯„åˆ†ï¼šé¢ç§¯50% + ä½ç½®30% + ç½®ä¿¡åº¦20%
            area_weight = 0.5
            position_weight = 0.3
            confidence_weight = 0.2
            
            # é¢ç§¯å½’ä¸€åŒ–
            if bbox_area > 0:
                normalized_area = min(np.log10(bbox_area + 1) / np.log10(max_possible_area + 1), 1.0)
            else:
                normalized_area = 0.0
            
            normalized_position = min(position_score, 1.0)
            normalized_confidence = min(avg_confidence, 1.0)
            
            composite_score = (
                area_weight * normalized_area +
                position_weight * normalized_position +
                confidence_weight * normalized_confidence
            )
            
            if composite_score > max_score:
                max_score = composite_score
                best_idx = i
                
        except Exception as e:
            continue
    
    if best_idx is None:
        return None, None
    
    # è¿”å›æœ€ä½³å€™é€‰äººçš„å…³é”®ç‚¹å’Œç½®ä¿¡åº¦
    best_keypoints = keypoints_data.xy[best_idx].cpu().numpy()
    best_confidence = keypoints_data.conf[best_idx].cpu().numpy()
    
    return best_keypoints, best_confidence

class VideoBadmintonDataset(Dataset):
    """
    VideoBadmintonæ•°æ®é›†åŠ è½½å™¨
    é›†æˆäº†æˆåŠŸçš„äººä½“æ£€æµ‹é€»è¾‘
    """
    
    def __init__(self, dataset_dir: str, max_samples_per_class: Optional[int] = None):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            dataset_dir: æ•°æ®é›†ç›®å½• (å¦‚ data/split/train/)
            max_samples_per_class: æ¯ä¸ªç±»åˆ«æœ€å¤§æ ·æœ¬æ•°ï¼Œç”¨äºé™åˆ¶æ•°æ®é‡
        """
        self.dataset_dir = dataset_dir
        self.max_samples_per_class = max_samples_per_class
        self.samples = self._collect_samples()
        
        print(f"ğŸ“ æ•°æ®é›†ç›®å½•: {dataset_dir}")
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(self.samples)}")
        self._print_class_distribution()
        
        # åˆå§‹åŒ–å§¿æ€æ£€æµ‹å™¨ï¼ˆåªç”¨äºæå–å…³é”®ç‚¹ï¼‰
        self.pose_model = YOLO('yolov8n-pose.pt')
    
    def _collect_samples(self) -> List[Tuple[str, int]]:
        """æ”¶é›†æ‰€æœ‰è§†é¢‘æ–‡ä»¶å’Œå¯¹åº”æ ‡ç­¾"""
        samples = []
        
        # éå†18ä¸ªåŠ¨ä½œç±»åˆ«æ–‡ä»¶å¤¹
        for class_id in range(18):
            # æŸ¥æ‰¾å¯¹åº”çš„ç±»åˆ«æ–‡ä»¶å¤¹
            class_folders = glob.glob(os.path.join(self.dataset_dir, f"{class_id:02d}_*"))
            
            if not class_folders:
                continue
            
            class_folder = class_folders[0]  # åº”è¯¥åªæœ‰ä¸€ä¸ªåŒ¹é…çš„æ–‡ä»¶å¤¹
            video_files = glob.glob(os.path.join(class_folder, "*.mp4"))
            
            # é™åˆ¶æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡ï¼ˆå¦‚æœæŒ‡å®šäº†ï¼‰
            if self.max_samples_per_class and len(video_files) > self.max_samples_per_class:
                video_files = random.sample(video_files, self.max_samples_per_class)
            
            # æ·»åŠ åˆ°æ ·æœ¬åˆ—è¡¨
            for video_file in video_files:
                samples.append((video_file, class_id))
        
        # éšæœºæ‰“ä¹±æ ·æœ¬é¡ºåº
        random.shuffle(samples)
        return samples
    
    def _print_class_distribution(self):
        """æ‰“å°ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡"""
        class_counts = {}
        for _, label in self.samples:
            class_counts[label] = class_counts.get(label, 0) + 1
        
        print("\nğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ:")
        for class_id in sorted(class_counts.keys()):
            class_name = RAW_CLASSES.get(class_id, f"Class_{class_id}")
            print(f"  {class_id:02d} - {class_name}: {class_counts[class_id]} ä¸ªæ ·æœ¬")
    
    def __len__(self) -> int:
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:
        """
        è·å–ä¸€ä¸ªè®­ç»ƒæ ·æœ¬
        
        Returns:
            (å…³é”®ç‚¹åºåˆ—å¼ é‡, åŠ¨ä½œç±»åˆ«æ ‡ç­¾)
        """
        video_path, label = self.samples[idx]
        
        # ä»è§†é¢‘ä¸­æå–å…³é”®ç‚¹åºåˆ—
        keypoints_sequence = self._extract_keypoints_from_video(video_path)
        
        if not keypoints_sequence:
            # å¦‚æœæå–å¤±è´¥ï¼Œè¿”å›é›¶å¼ é‡
            zero_tensor = torch.zeros(MODEL_CONFIG['keypoints'] * 2 * MODEL_CONFIG['sequence_length'])
            return zero_tensor, label
        
        # è½¬æ¢ä¸ºå›ºå®šé•¿åº¦çš„å¼ é‡
        sequence_tensor = self._keypoints_to_tensor(keypoints_sequence)
        return sequence_tensor, label
    
    def _extract_keypoints_from_video(self, video_path: str) -> List[Keypoints]:
        """
        ä»è§†é¢‘æå–å…³é”®ç‚¹åºåˆ— - é›†æˆtest_person.pyçš„æˆåŠŸå®ç°
        
        ä¼˜åŒ–ç‚¹ï¼š
        1. æ™ºèƒ½è£å‰ªå»é™¤è¿‡å¤šèƒŒæ™¯
        2. å›¾åƒé¢„å¤„ç†æé«˜æ£€æµ‹ç‡
        3. ä¼˜å…ˆé€‰æ‹©é è¿‘æ‘„åƒå¤´çš„äººï¼ˆæ ¸å¿ƒæ”¹è¿›ï¼‰
        4. å®‰å…¨çš„é”™è¯¯å¤„ç†
        """
        cap = cv2.VideoCapture(video_path)
        keypoints_list = []
        
        if not cap.isOpened():
            return []
        
        frame_count = 0
        max_frames = TRAINING_CONFIG['max_frames_per_video']
        successful_detections = 0
        
        while frame_count < max_frames:
            ret, frame = cap.read()
            if not ret:
                break
            
            try:
                # ğŸ“ æ™ºèƒ½è£å‰ªï¼šå»é™¤è¿‡å¤šèƒŒæ™¯ï¼Œçªå‡ºäººç‰©åŒºåŸŸ
                height, width = frame.shape[:2]
                
                # è£å‰ªå‚æ•°ï¼šä¿ç•™ä¸­å¤®åŒºåŸŸï¼Œå»æ‰ä¸Šä¸‹å·¦å³çš„èƒŒæ™¯
                crop_y1, crop_y2 = int(height * 0.15), int(height * 0.90)
                crop_x1, crop_x2 = int(width * 0.10), int(width * 0.90)
                cropped_frame = frame[crop_y1:crop_y2, crop_x1:crop_x2]
                
                # ğŸ”§ å›¾åƒé¢„å¤„ç†ï¼šæé«˜å¯¹æ¯”åº¦å’Œæ¸…æ™°åº¦
                processed_frame = preprocess_frame(cropped_frame)
                
                # ğŸ“ è°ƒæ•´åˆ°æ¨¡å‹è¾“å…¥å°ºå¯¸
                target_size = 640
                frame_resized = cv2.resize(processed_frame, (target_size, target_size))
                
                # ğŸ¤– YOLOv8å§¿æ€æ£€æµ‹ï¼šé™ä½æ£€æµ‹é˜ˆå€¼æé«˜å¬å›ç‡
                results = self.pose_model(frame_resized, verbose=False, conf=0.1)
                
                # ğŸ¯ æ ¸å¿ƒæ”¹è¿›ï¼šä½¿ç”¨test_person.pyä¸­æˆåŠŸçš„äººä½“é€‰æ‹©ç®—æ³•
                best_keypoints, best_confidence = select_nearest_person_keypoints(
                    results, target_size, target_size
                )
                
                if best_keypoints is not None and best_confidence is not None:
                    # æ£€æŸ¥æ•´ä½“è´¨é‡ï¼šå¹³å‡ç½®ä¿¡åº¦å¿…é¡»è¾¾åˆ°æœ€ä½è¦æ±‚
                    avg_confidence = safe_float(best_confidence.mean())
                    if avg_confidence > 0.05:  # é™ä½é˜ˆå€¼ï¼Œæé«˜æ£€æµ‹æˆåŠŸç‡
                        keypoints_list.append(Keypoints(
                            points=best_keypoints,
                            confidence=best_confidence
                        ))
                        successful_detections += 1
                
            except Exception as e:
                # è·³è¿‡å¤„ç†å¤±è´¥çš„å¸§ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€å¸§
                pass
            
            frame_count += 1
            
            # ğŸ¯ ä¼˜åŒ–ï¼šå¦‚æœæ£€æµ‹æˆåŠŸç‡å¤ªä½ï¼Œæå‰ç»ˆæ­¢
            if frame_count > 20 and successful_detections == 0:
                break
        
        cap.release()
        
        # ğŸ“Š æ£€æµ‹ç»Ÿè®¡ï¼ˆåªå¯¹å¤±è´¥ç‡é«˜çš„è§†é¢‘è¾“å‡ºè­¦å‘Šï¼‰
        success_rate = successful_detections / max(frame_count, 1) * 100
        if success_rate < 10:  # æˆåŠŸç‡ä½äº10%æ—¶è¾“å‡ºè­¦å‘Š
            video_name = os.path.basename(video_path)
            print(f"âš ï¸  {video_name}: æ£€æµ‹æˆåŠŸç‡ {success_rate:.1f}% ({successful_detections}/{frame_count})")
        
        return keypoints_list
    
    def _keypoints_to_tensor(self, keypoints_sequence: List[Keypoints]) -> torch.Tensor:
        """
        å°†å…³é”®ç‚¹åºåˆ—è½¬æ¢ä¸ºå›ºå®šé•¿åº¦çš„å¼ é‡
        
        Args:
            keypoints_sequence: å…³é”®ç‚¹åºåˆ—
            
        Returns:
            å›ºå®šé•¿åº¦çš„å¼ é‡ (340ç»´: 17å…³é”®ç‚¹ Ã— 2åæ ‡ Ã— 10å¸§)
        """
        target_length = MODEL_CONFIG['sequence_length']
        
        if len(keypoints_sequence) >= target_length:
            # å¦‚æœåºåˆ—è¿‡é•¿ï¼Œå‡åŒ€é‡‡æ ·é€‰æ‹©ä»£è¡¨æ€§å¸§
            indices = np.linspace(0, len(keypoints_sequence) - 1, target_length, dtype=int)
            selected_keypoints = [keypoints_sequence[i] for i in indices]
        else:
            # å¦‚æœåºåˆ—è¿‡çŸ­ï¼Œé‡‡ç”¨é‡å¤å¡«å……ç­–ç•¥
            selected_keypoints = keypoints_sequence.copy()
            while len(selected_keypoints) < target_length:
                if keypoints_sequence:
                    # é‡å¤æœ€åä¸€å¸§
                    selected_keypoints.append(keypoints_sequence[-1])
                else:
                    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå…³é”®ç‚¹ï¼Œç”¨é›¶å¡«å……
                    zero_keypoints = Keypoints(
                        points=np.zeros((MODEL_CONFIG['keypoints'], 2)),
                        confidence=np.zeros(MODEL_CONFIG['keypoints'])
                    )
                    selected_keypoints.append(zero_keypoints)
        
        # ğŸ”„ è½¬æ¢ä¸ºå¼ é‡ï¼šå±•å¹³æ‰€æœ‰å…³é”®ç‚¹åæ ‡
        sequence_data = []
        for keypoints in selected_keypoints:
            sequence_data.extend(keypoints.points.flatten())
        
        return torch.FloatTensor(sequence_data)

class Trainer:
    """
    ç¾½æ¯›çƒåŠ¨ä½œåˆ†ç±»æ¨¡å‹è®­ç»ƒå™¨
    """
    
    def __init__(self, data_root: str = "data/split/"):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            data_root: åˆ†å‰²åçš„æ•°æ®é›†æ ¹ç›®å½•
        """
        self.data_root = data_root
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print("ğŸš€ åˆå§‹åŒ–ç¾½æ¯›çƒåŠ¨ä½œåˆ†ç±»è®­ç»ƒå™¨")
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"ğŸ“ æ•°æ®æ ¹ç›®å½•: {data_root}")
        print("ğŸ¯ é›†æˆäº†ä¼˜åŒ–çš„äººä½“é€‰æ‹©ç®—æ³•")
        
        # éªŒè¯æ•°æ®ç›®å½•
        self._validate_data_directories()
    
    def _validate_data_directories(self):
        """éªŒè¯æ•°æ®ç›®å½•ç»“æ„"""
        required_dirs = ['train', 'val', 'test']
        for dir_name in required_dirs:
            dir_path = os.path.join(self.data_root, dir_name)
            if not os.path.exists(dir_path):
                raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°å¿…éœ€çš„ç›®å½•: {dir_path}")
        print("âœ… æ•°æ®ç›®å½•ç»“æ„éªŒè¯é€šè¿‡")
    
    def train(self, epochs: int = TRAINING_CONFIG['max_epochs'], 
              save_path: str = "badminton_model.pth"):
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            epochs: è®­ç»ƒè½®æ•°
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        """
        print("=" * 60)
        print("ğŸ¸ å¼€å§‹è®­ç»ƒç¾½æ¯›çƒåŠ¨ä½œåˆ†ç±»æ¨¡å‹")
        print("=" * 60)
        
        training_start_time = time.time()
        
        # åŠ è½½æ•°æ®é›†
        print("ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®é›†...")
        train_dataset = VideoBadmintonDataset(f"{self.data_root}/train/")
        val_dataset = VideoBadmintonDataset(f"{self.data_root}/val/")
        test_dataset = VideoBadmintonDataset(f"{self.data_root}/test/")
        
        print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ")
        print(f"   è®­ç»ƒé›†: {len(train_dataset)} ä¸ªæ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(val_dataset)} ä¸ªæ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(test_dataset)} ä¸ªæ ·æœ¬")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=MODEL_CONFIG['batch_size'],
            shuffle=True,
            num_workers=2,
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=MODEL_CONFIG['batch_size'],
            shuffle=False,
            num_workers=2,
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        # åˆå§‹åŒ–æ¨¡å‹
        detector = BadmintonDetector()
        model = detector.action_classifier.to(self.device)
        
        # è®­ç»ƒé…ç½®
        criterion = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=MODEL_CONFIG['learning_rate'],
            weight_decay=MODEL_CONFIG['weight_decay']
        )
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            patience=TRAINING_CONFIG['lr_scheduler_patience'],
            factor=0.5
        )
        
        # è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
        best_val_accuracy = 0.0
        best_val_loss = float('inf')
        epochs_without_improvement = 0
        training_history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'learning_rate': []
        }
        
        print(f"ğŸ”§ è®­ç»ƒé…ç½®:")
        print(f"   æ‰¹æ¬¡å¤§å°: {MODEL_CONFIG['batch_size']}")
        print(f"   å­¦ä¹ ç‡: {MODEL_CONFIG['learning_rate']}")
        print(f"   æƒé‡è¡°å‡: {MODEL_CONFIG['weight_decay']}")
        print(f"   æœ€å¤§è½®æ•°: {epochs}")
        print("-" * 40)
        
        # ä¸»è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            epoch_start_time = time.time()
            
            print(f"\nğŸ“… Epoch {epoch+1}/{epochs}")
            print("-" * 30)
            
            # è®­ç»ƒé˜¶æ®µ
            train_loss, train_acc = self._train_epoch(model, train_loader, criterion, optimizer)
            
            # éªŒè¯é˜¶æ®µ
            val_loss, val_acc = self._validate_epoch(model, val_loader, criterion)
            
            # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
            scheduler.step(val_loss)
            current_lr = optimizer.param_groups[0]['lr']
            
            # è®°å½•è®­ç»ƒå†å²
            training_history['train_loss'].append(train_loss)
            training_history['train_acc'].append(train_acc)
            training_history['val_loss'].append(val_loss)
            training_history['val_acc'].append(val_acc)
            training_history['learning_rate'].append(current_lr)
            
            # æ˜¾ç¤ºå½“å‰è½®ç»“æœ
            epoch_time = time.time() - epoch_start_time
            print(f"ğŸ“Š è®­ç»ƒæŸå¤±: {train_loss:.4f} | è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}%")
            print(f"ğŸ“Š éªŒè¯æŸå¤±: {val_loss:.4f} | éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
            print(f"â±ï¸  è½®æ¬¡è€—æ—¶: {epoch_time:.1f}ç§’ | å­¦ä¹ ç‡: {current_lr:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_accuracy:
                best_val_accuracy = val_acc
                best_val_loss = val_loss
                epochs_without_improvement = 0
                
                if TRAINING_CONFIG['save_best_only']:
                    torch.save(model.state_dict(), save_path)
                    print(f"ğŸ¯ æ–°çš„æœ€ä½³æ¨¡å‹ï¼éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}% (å·²ä¿å­˜)")
            else:
                epochs_without_improvement += 1
            
            # æ—©åœæ£€æŸ¥
            if epochs_without_improvement >= TRAINING_CONFIG['early_stopping_patience']:
                print(f"â¹ï¸  æ—©åœè§¦å‘ï¼{epochs_without_improvement} è½®æ— æ”¹è¿›")
                break
        
        # è®­ç»ƒç»“æŸç»Ÿè®¡
        total_training_time = time.time() - training_start_time
        print("\n" + "=" * 60)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_training_time:.1f}ç§’")
        print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_accuracy:.2f}%")
        print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = save_path.replace('.pth', '_history.json')
        with open(history_path, 'w') as f:
            json.dump(training_history, f, indent=2)
        print(f"ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜è‡³: {history_path}")
        
        # æœ€ç»ˆæµ‹è¯•è¯„ä¼°
        if os.path.exists(save_path):
            self._final_evaluation(test_dataset, save_path)
    
    def _train_epoch(self, model, train_loader, criterion, optimizer) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            # æ˜¾ç¤ºè¿›åº¦
            if batch_idx % 20 == 0:  # å‡å°‘è¾“å‡ºé¢‘ç‡
                current_acc = 100.0 * correct / total
                print(f"  ğŸ“¦ æ‰¹æ¬¡ {batch_idx}/{len(train_loader)} | "
                      f"æŸå¤±: {loss.item():.4f} | å‡†ç¡®ç‡: {current_acc:.2f}%")
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100.0 * correct / total
        return avg_loss, accuracy
    
    def _validate_epoch(self, model, val_loader, criterion) -> Tuple[float, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = model(data)
                loss = criterion(output, target)
                
                total_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()
        
        avg_loss = total_loss / len(val_loader)
        accuracy = 100.0 * correct / total
        return avg_loss, accuracy
    
    def _final_evaluation(self, test_dataset, model_path):
        """æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°"""
        print("\n" + "=" * 60)
        print("ğŸ§ª æœ€ç»ˆè¯„ä¼° (ä½¿ç”¨æµ‹è¯•é›†)")
        print("=" * 60)
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=MODEL_CONFIG['batch_size'],
            shuffle=False
        )
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        detector = BadmintonDetector()
        model = detector.action_classifier.to(self.device)
        model.load_state_dict(torch.load(model_path, map_location=self.device))
        model.eval()
        
        # è¯„ä¼°
        correct = 0
        total = 0
        class_correct = [0] * 18
        class_total =  [0] * 18
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = model(data)
                _, predicted = torch.max(output, 1)
                
                total += target.size(0)
                correct += (predicted == target).sum().item()
                
                # è®°å½•é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
                all_predictions.extend(predicted.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
                
                # æŒ‰ç±»åˆ«ç»Ÿè®¡
                for i in range(target.size(0)):
                    label = target[i].item()
                    class_total[label] += 1
                    if predicted[i] == target[i]:
                        class_correct[label] += 1
        
        # è¾“å‡ºæ•´ä½“ç»“æœ
        overall_accuracy = 100.0 * correct / total
        print(f"ğŸ¯ æµ‹è¯•é›†æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy:.2f}% ({correct}/{total})")
        
        # è¾“å‡ºå„ç±»åˆ«è¯¦ç»†ç»“æœ
        print("\nğŸ“Š å„ç±»åˆ«è¯¦ç»†ç»“æœ:")
        print("-" * 80)
        print(f"{'ç±»åˆ«ID':<6} {'åŠ¨ä½œåç§°':<20} {'å‡†ç¡®ç‡':<10} {'æ ·æœ¬æ•°':<10}")
        print("-" * 80)
        
        for i in range(18):
            if class_total[i] > 0:
                acc = 100.0 * class_correct[i] / class_total[i]
                class_name = RAW_CLASSES.get(i, f"Class_{i}")
                print(f"{i:02d}     {class_name:<20} {acc:>6.2f}%     {class_total[i]:>4d}")
        
        print("-" * 80)
        print("âœ… æœ€ç»ˆè¯„ä¼°å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è®­ç»ƒç¾½æ¯›çƒåŠ¨ä½œåˆ†ç±»æ¨¡å‹")
    parser.add_argument("--data", default="data/split/", 
                       help="åˆ†å‰²åçš„æ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument("--epochs", type=int, default=TRAINING_CONFIG['max_epochs'], 
                       help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--output", default="badminton_model.pth", 
                       help="æ¨¡å‹è¾“å‡ºè·¯å¾„")
    parser.add_argument("--batch-size", type=int, default=MODEL_CONFIG['batch_size'], 
                       help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=MODEL_CONFIG['learning_rate'], 
                       help="å­¦ä¹ ç‡")
    
    args = parser.parse_args()
    
    # æ›´æ–°é…ç½®
    MODEL_CONFIG['batch_size'] = args.batch_size
    MODEL_CONFIG['learning_rate'] = args.lr
    
    # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    
    # å¼€å§‹è®­ç»ƒ
    trainer = Trainer(args.data)
    trainer.train(epochs=args.epochs, save_path=args.output)

if __name__ == "__main__":
    main()
