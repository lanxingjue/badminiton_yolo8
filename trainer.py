"""
æ¨¡å‹è®­ç»ƒå™¨ - è®­ç»ƒç¾½æ¯›çƒåŠ¨ä½œåˆ†ç±»æ¨¡å‹
LinusåŸåˆ™ï¼šå·¥å…·è¦ç®€å•ã€å¯é ã€å¯é¢„æµ‹
é›†æˆäº†æˆåŠŸçš„äººä½“é€‰æ‹©ç®—æ³• + GPUä¼˜åŒ–æ”¯æŒ
"""

import os
import glob
import cv2
import torch
import torch.cuda.amp as amp  # ğŸ”§ æ··åˆç²¾åº¦è®­ç»ƒ
import numpy as np
from torch.utils.data import Dataset, DataLoader
from typing import List, Tuple, Optional
import random
import time
import json
from datetime import datetime
from ultralytics import YOLO

from detector import BadmintonDetector
from core import Keypoints
from config import MODEL_CONFIG, TRAINING_CONFIG, RAW_CLASSES

def get_optimal_config():
    """
    æ ¹æ®ç¡¬ä»¶è‡ªåŠ¨ä¼˜åŒ–é…ç½®
    é’ˆå¯¹RTX 4090ç­‰é«˜ç«¯GPUè¿›è¡Œä¼˜åŒ–
    """
    if torch.cuda.is_available():
        device = torch.device('cuda')
        gpu_name = torch.cuda.get_device_name(0)
        vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        print(f"ğŸš€ æ£€æµ‹åˆ°GPU: {gpu_name}")
        print(f"ğŸ’¾ æ˜¾å­˜: {vram_gb:.1f}GB")
        
        # RTX 4090ä¼˜åŒ–é…ç½®
        if "4090" in gpu_name or "4080" in gpu_name:
            batch_size = 128  # 4090å¯ä»¥ç”¨æ›´å¤§æ‰¹æ¬¡
            num_workers = 12
            prefetch_factor = 4
            print("ğŸ¯ ä½¿ç”¨RTX 4090ä¼˜åŒ–é…ç½®")
        elif "3090" in gpu_name or "3080" in gpu_name:
            batch_size = 96
            num_workers = 8
            prefetch_factor = 3
            print("ğŸ¯ ä½¿ç”¨RTX 30ç³»ä¼˜åŒ–é…ç½®")
        elif "2080" in gpu_name or "2070" in gpu_name:
            batch_size = 64
            num_workers = 6
            prefetch_factor = 2
            print("ğŸ¯ ä½¿ç”¨RTX 20ç³»ä¼˜åŒ–é…ç½®")
        else:
            batch_size = 32
            num_workers = 4
            prefetch_factor = 2
            print("ğŸ¯ ä½¿ç”¨é€šç”¨GPUé…ç½®")
            
        return {
            'device': device,
            'batch_size': batch_size,
            'num_workers': num_workers,
            'prefetch_factor': prefetch_factor,
            'pin_memory': True,
            'mixed_precision': True,
            'persistent_workers': True
        }
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUé…ç½®")
        return {
            'device': torch.device('cpu'),
            'batch_size': 16,
            'num_workers': 2,
            'prefetch_factor': 2,
            'pin_memory': False,
            'mixed_precision': False,
            'persistent_workers': False
        }

def preprocess_frame(frame):
    """
    æé«˜å›¾åƒè´¨é‡ï¼Œå¢å¼ºäººä½“æ£€æµ‹æ•ˆæœ
    """
    # æé«˜å¯¹æ¯”åº¦å’Œäº®åº¦
    frame = cv2.convertScaleAbs(frame, alpha=1.2, beta=10)
    # é”åŒ–æ»¤æ³¢å»é™¤è¿åŠ¨æ¨¡ç³Š
    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
    frame = cv2.filter2D(frame, -1, kernel)
    return frame

def safe_float(value):
    """
    å®‰å…¨çš„æ•°å€¼è½¬æ¢ï¼Œå¤„ç†æ‰€æœ‰numpyç±»å‹
    é›†æˆè‡ªtest_person.pyçš„æˆåŠŸå®ç°
    """
    if isinstance(value, (np.ndarray, np.generic)):
        if hasattr(value, 'item'):
            return float(value.item())
        elif len(value.shape) == 0:  # 0ç»´æ•°ç»„ï¼ˆæ ‡é‡ï¼‰
            return float(value)
        elif value.size == 1:  # åªæœ‰ä¸€ä¸ªå…ƒç´ 
            return float(value.flat[0])
        else:
            raise ValueError(f"Cannot convert array of size {value.size} to scalar")
    else:
        return float(value)

def select_nearest_person_keypoints(results, frame_height=640, frame_width=640):
    """
    ä»YOLOv8å§¿æ€æ£€æµ‹ç»“æœä¸­é€‰æ‹©æœ€é è¿‘æ‘„åƒå¤´çš„äºº
    é›†æˆè‡ªtest_person.pyçš„æˆåŠŸå®ç°ï¼Œä¸“é—¨é’ˆå¯¹è®­ç»ƒæ•°æ®ä¼˜åŒ–
    
    åˆ¤æ–­æ ‡å‡†ï¼š
    1. å…³é”®ç‚¹åŒ…å›´ç›’é¢ç§¯æœ€å¤§ï¼ˆäººä½“åœ¨ç”»é¢ä¸­æœ€å¤§ï¼‰
    2. å…³é”®ç‚¹è´¨å¿ƒä½ç½®æœ€é è¿‘ç”»é¢åº•éƒ¨ï¼ˆæ›´é è¿‘æ‘„åƒå¤´ï¼‰
    3. ç»¼åˆè¯„åˆ†é€‰æ‹©æœ€ä½³å€™é€‰
    """
    if not results or len(results) == 0:
        return None, None
    
    result = results[0]
    
    if not hasattr(result, 'keypoints') or result.keypoints is None:
        return None, None
    
    keypoints_data = result.keypoints
    
    if len(keypoints_data.xy) == 0:
        return None, None
    
    best_idx = None
    max_score = 0.0
    max_possible_area = float(frame_height * frame_width)
    
    for i in range(len(keypoints_data.xy)):
        try:
            coords = keypoints_data.xy[i].cpu().numpy().astype(np.float64)
            confidence = keypoints_data.conf[i].cpu().numpy().astype(np.float64)
            
            # è¿‡æ»¤ä½ç½®ä¿¡åº¦å…³é”®ç‚¹
            valid_mask = confidence > 0.3
            valid_points = coords[valid_mask]
            valid_conf = confidence[valid_mask]
            
            if len(valid_points) < 5:  # è‡³å°‘éœ€è¦5ä¸ªé«˜ç½®ä¿¡åº¦å…³é”®ç‚¹
                continue
            
            # è®¡ç®—åŒ…å›´ç›’é¢ç§¯
            min_xy = valid_points.min(axis=0)
            max_xy = valid_points.max(axis=0)
            
            bbox_width = safe_float(max_xy[0] - min_xy[0])
            bbox_height = safe_float(max_xy[1] - min_xy[1])
            bbox_area = bbox_width * bbox_height
            
            # è®¡ç®—è´¨å¿ƒä½ç½®
            centroid_y = safe_float(valid_points[:, 1].mean())
            position_score = centroid_y / frame_height
            
            # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
            avg_confidence = safe_float(valid_conf.mean())
            
            # ç»¼åˆè¯„åˆ†ï¼šé¢ç§¯50% + ä½ç½®30% + ç½®ä¿¡åº¦20%
            area_weight = 0.5
            position_weight = 0.3
            confidence_weight = 0.2
            
            # é¢ç§¯å½’ä¸€åŒ–
            if bbox_area > 0:
                normalized_area = min(np.log10(bbox_area + 1) / np.log10(max_possible_area + 1), 1.0)
            else:
                normalized_area = 0.0
            
            normalized_position = min(position_score, 1.0)
            normalized_confidence = min(avg_confidence, 1.0)
            
            composite_score = (
                area_weight * normalized_area +
                position_weight * normalized_position +
                confidence_weight * normalized_confidence
            )
            
            if composite_score > max_score:
                max_score = composite_score
                best_idx = i
                
        except Exception as e:
            continue
    
    if best_idx is None:
        return None, None
    
    # è¿”å›æœ€ä½³å€™é€‰äººçš„å…³é”®ç‚¹å’Œç½®ä¿¡åº¦
    best_keypoints = keypoints_data.xy[best_idx].cpu().numpy()
    best_confidence = keypoints_data.conf[best_idx].cpu().numpy()
    
    return best_keypoints, best_confidence

class VideoBadmintonDataset(Dataset):
    """
    VideoBadmintonæ•°æ®é›†åŠ è½½å™¨
    é›†æˆäº†æˆåŠŸçš„äººä½“æ£€æµ‹é€»è¾‘ + GPUä¼˜åŒ–
    """
    
    def __init__(self, dataset_dir: str, max_samples_per_class: Optional[int] = None, use_gpu: bool = True):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            dataset_dir: æ•°æ®é›†ç›®å½• (å¦‚ data/split/train/)
            max_samples_per_class: æ¯ä¸ªç±»åˆ«æœ€å¤§æ ·æœ¬æ•°ï¼Œç”¨äºé™åˆ¶æ•°æ®é‡
            use_gpu: æ˜¯å¦ä½¿ç”¨GPUä¼˜åŒ–æ¨¡å‹
        """
        self.dataset_dir = dataset_dir
        self.max_samples_per_class = max_samples_per_class
        self.samples = self._collect_samples()
        
        print(f"ğŸ“ æ•°æ®é›†ç›®å½•: {dataset_dir}")
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(self.samples)}")
        self._print_class_distribution()
        
        # ğŸ”§ GPUä¼˜åŒ–ï¼šæ ¹æ®ç¡¬ä»¶é€‰æ‹©æ›´åˆé€‚çš„YOLOv8æ¨¡å‹
        if use_gpu and torch.cuda.is_available():
            # GPUæ¨¡å¼ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼Œæ£€æµ‹ç²¾åº¦æ›´é«˜
            self.pose_model = YOLO('yolov8m-pose.pt')
            print("ğŸ¯ GPUæ¨¡å¼ï¼šä½¿ç”¨YOLOv8m-poseæ¨¡å‹")
        else:
            # CPUæ¨¡å¼ä½¿ç”¨è½»é‡çº§æ¨¡å‹
            self.pose_model = YOLO('yolov8n-pose.pt')
            print("ğŸ¯ CPUæ¨¡å¼ï¼šä½¿ç”¨YOLOv8n-poseæ¨¡å‹")
    
    def _collect_samples(self) -> List[Tuple[str, int]]:
        """æ”¶é›†æ‰€æœ‰è§†é¢‘æ–‡ä»¶å’Œå¯¹åº”æ ‡ç­¾"""
        samples = []
        
        # éå†18ä¸ªåŠ¨ä½œç±»åˆ«æ–‡ä»¶å¤¹
        for class_id in range(18):
            # æŸ¥æ‰¾å¯¹åº”çš„ç±»åˆ«æ–‡ä»¶å¤¹
            class_folders = glob.glob(os.path.join(self.dataset_dir, f"{class_id:02d}_*"))
            
            if not class_folders:
                continue
            
            class_folder = class_folders[0]  # åº”è¯¥åªæœ‰ä¸€ä¸ªåŒ¹é…çš„æ–‡ä»¶å¤¹
            video_files = glob.glob(os.path.join(class_folder, "*.mp4"))
            
            # é™åˆ¶æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡ï¼ˆå¦‚æœæŒ‡å®šäº†ï¼‰
            if self.max_samples_per_class and len(video_files) > self.max_samples_per_class:
                video_files = random.sample(video_files, self.max_samples_per_class)
            
            # æ·»åŠ åˆ°æ ·æœ¬åˆ—è¡¨
            for video_file in video_files:
                samples.append((video_file, class_id))
        
        # éšæœºæ‰“ä¹±æ ·æœ¬é¡ºåº
        random.shuffle(samples)
        return samples
    
    def _print_class_distribution(self):
        """æ‰“å°ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡"""
        class_counts = {}
        for _, label in self.samples:
            class_counts[label] = class_counts.get(label, 0) + 1
        
        print("\nğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ:")
        for class_id in sorted(class_counts.keys()):
            class_name = RAW_CLASSES.get(class_id, f"Class_{class_id}")
            print(f"  {class_id:02d} - {class_name}: {class_counts[class_id]} ä¸ªæ ·æœ¬")
    
    def __len__(self) -> int:
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:
        """
        è·å–ä¸€ä¸ªè®­ç»ƒæ ·æœ¬
        
        Returns:
            (å…³é”®ç‚¹åºåˆ—å¼ é‡, åŠ¨ä½œç±»åˆ«æ ‡ç­¾)
        """
        video_path, label = self.samples[idx]
        
        # ä»è§†é¢‘ä¸­æå–å…³é”®ç‚¹åºåˆ—
        keypoints_sequence = self._extract_keypoints_from_video(video_path)
        
        if not keypoints_sequence:
            # å¦‚æœæå–å¤±è´¥ï¼Œè¿”å›é›¶å¼ é‡
            zero_tensor = torch.zeros(MODEL_CONFIG['keypoints'] * 2 * MODEL_CONFIG['sequence_length'])
            return zero_tensor, label
        
        # è½¬æ¢ä¸ºå›ºå®šé•¿åº¦çš„å¼ é‡
        sequence_tensor = self._keypoints_to_tensor(keypoints_sequence)
        return sequence_tensor, label
    
    def _extract_keypoints_from_video(self, video_path: str) -> List[Keypoints]:
        """
        ä»è§†é¢‘æå–å…³é”®ç‚¹åºåˆ— - é›†æˆtest_person.pyçš„æˆåŠŸå®ç°
        
        ä¼˜åŒ–ç‚¹ï¼š
        1. æ™ºèƒ½è£å‰ªå»é™¤è¿‡å¤šèƒŒæ™¯
        2. å›¾åƒé¢„å¤„ç†æé«˜æ£€æµ‹ç‡
        3. ä¼˜å…ˆé€‰æ‹©é è¿‘æ‘„åƒå¤´çš„äººï¼ˆæ ¸å¿ƒæ”¹è¿›ï¼‰
        4. å®‰å…¨çš„é”™è¯¯å¤„ç†
        """
        cap = cv2.VideoCapture(video_path)
        keypoints_list = []
        
        if not cap.isOpened():
            return []
        
        frame_count = 0
        max_frames = TRAINING_CONFIG['max_frames_per_video']
        successful_detections = 0
        
        while frame_count < max_frames:
            ret, frame = cap.read()
            if not ret:
                break
            
            try:
                # ğŸ“ æ™ºèƒ½è£å‰ªï¼šå»é™¤è¿‡å¤šèƒŒæ™¯ï¼Œçªå‡ºäººç‰©åŒºåŸŸ
                height, width = frame.shape[:2]
                
                # è£å‰ªå‚æ•°ï¼šä¿ç•™ä¸­å¤®åŒºåŸŸï¼Œå»æ‰ä¸Šä¸‹å·¦å³çš„èƒŒæ™¯
                crop_y1, crop_y2 = int(height * 0.15), int(height * 0.90)
                crop_x1, crop_x2 = int(width * 0.10), int(width * 0.90)
                cropped_frame = frame[crop_y1:crop_y2, crop_x1:crop_x2]
                
                # ğŸ”§ å›¾åƒé¢„å¤„ç†ï¼šæé«˜å¯¹æ¯”åº¦å’Œæ¸…æ™°åº¦
                processed_frame = preprocess_frame(cropped_frame)
                
                # ğŸ“ è°ƒæ•´åˆ°æ¨¡å‹è¾“å…¥å°ºå¯¸
                target_size = 640
                frame_resized = cv2.resize(processed_frame, (target_size, target_size))
                
                # ğŸ¤– YOLOv8å§¿æ€æ£€æµ‹ï¼šé™ä½æ£€æµ‹é˜ˆå€¼æé«˜å¬å›ç‡
                results = self.pose_model(frame_resized, verbose=False, conf=0.1)
                
                # ğŸ¯ æ ¸å¿ƒæ”¹è¿›ï¼šä½¿ç”¨test_person.pyä¸­æˆåŠŸçš„äººä½“é€‰æ‹©ç®—æ³•
                best_keypoints, best_confidence = select_nearest_person_keypoints(
                    results, target_size, target_size
                )
                
                if best_keypoints is not None and best_confidence is not None:
                    # æ£€æŸ¥æ•´ä½“è´¨é‡ï¼šå¹³å‡ç½®ä¿¡åº¦å¿…é¡»è¾¾åˆ°æœ€ä½è¦æ±‚
                    avg_confidence = safe_float(best_confidence.mean())
                    if avg_confidence > 0.05:  # é™ä½é˜ˆå€¼ï¼Œæé«˜æ£€æµ‹æˆåŠŸç‡
                        keypoints_list.append(Keypoints(
                            points=best_keypoints,
                            confidence=best_confidence
                        ))
                        successful_detections += 1
                
            except Exception as e:
                # è·³è¿‡å¤„ç†å¤±è´¥çš„å¸§ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€å¸§
                pass
            
            frame_count += 1
            
            # ğŸ¯ ä¼˜åŒ–ï¼šå¦‚æœæ£€æµ‹æˆåŠŸç‡å¤ªä½ï¼Œæå‰ç»ˆæ­¢
            if frame_count > 20 and successful_detections == 0:
                break
        
        cap.release()
        
        # ğŸ“Š æ£€æµ‹ç»Ÿè®¡ï¼ˆåªå¯¹å¤±è´¥ç‡é«˜çš„è§†é¢‘è¾“å‡ºè­¦å‘Šï¼‰
        success_rate = successful_detections / max(frame_count, 1) * 100
        if success_rate < 10:  # æˆåŠŸç‡ä½äº10%æ—¶è¾“å‡ºè­¦å‘Š
            video_name = os.path.basename(video_path)
            print(f"âš ï¸  {video_name}: æ£€æµ‹æˆåŠŸç‡ {success_rate:.1f}% ({successful_detections}/{frame_count})")
        
        return keypoints_list
    
    def _keypoints_to_tensor(self, keypoints_sequence: List[Keypoints]) -> torch.Tensor:
        """
        å°†å…³é”®ç‚¹åºåˆ—è½¬æ¢ä¸ºå›ºå®šé•¿åº¦çš„å¼ é‡
        
        Args:
            keypoints_sequence: å…³é”®ç‚¹åºåˆ—
            
        Returns:
            å›ºå®šé•¿åº¦çš„å¼ é‡ (340ç»´: 17å…³é”®ç‚¹ Ã— 2åæ ‡ Ã— 10å¸§)
        """
        target_length = MODEL_CONFIG['sequence_length']
        
        if len(keypoints_sequence) >= target_length:
            # å¦‚æœåºåˆ—è¿‡é•¿ï¼Œå‡åŒ€é‡‡æ ·é€‰æ‹©ä»£è¡¨æ€§å¸§
            indices = np.linspace(0, len(keypoints_sequence) - 1, target_length, dtype=int)
            selected_keypoints = [keypoints_sequence[i] for i in indices]
        else:
            # å¦‚æœåºåˆ—è¿‡çŸ­ï¼Œé‡‡ç”¨é‡å¤å¡«å……ç­–ç•¥
            selected_keypoints = keypoints_sequence.copy()
            while len(selected_keypoints) < target_length:
                if keypoints_sequence:
                    # é‡å¤æœ€åä¸€å¸§
                    selected_keypoints.append(keypoints_sequence[-1])
                else:
                    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå…³é”®ç‚¹ï¼Œç”¨é›¶å¡«å……
                    zero_keypoints = Keypoints(
                        points=np.zeros((MODEL_CONFIG['keypoints'], 2)),
                        confidence=np.zeros(MODEL_CONFIG['keypoints'])
                    )
                    selected_keypoints.append(zero_keypoints)
        
        # ğŸ”„ è½¬æ¢ä¸ºå¼ é‡ï¼šå±•å¹³æ‰€æœ‰å…³é”®ç‚¹åæ ‡
        sequence_data = []
        for keypoints in selected_keypoints:
            sequence_data.extend(keypoints.points.flatten())
        
        return torch.FloatTensor(sequence_data)

class Trainer:
    """
    ç¾½æ¯›çƒåŠ¨ä½œåˆ†ç±»æ¨¡å‹è®­ç»ƒå™¨ - GPUä¼˜åŒ–ç‰ˆæœ¬
    """
    
    def __init__(self, data_root: str = "data/split/", force_cpu: bool = False):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            data_root: åˆ†å‰²åçš„æ•°æ®é›†æ ¹ç›®å½•
            force_cpu: å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼
        """
        self.data_root = data_root
        
        # ğŸ”§ GPUä¼˜åŒ–é…ç½®
        if force_cpu:
            self.config = {
                'device': torch.device('cpu'),
                'batch_size': 16,
                'num_workers': 2,
                'prefetch_factor': 2,
                'pin_memory': False,
                'mixed_precision': False,
                'persistent_workers': False
            }
            print("ğŸ”§ å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼")
        else:
            self.config = get_optimal_config()
        
        self.device = self.config['device']
        
        print("ğŸš€ åˆå§‹åŒ–ç¾½æ¯›çƒåŠ¨ä½œåˆ†ç±»è®­ç»ƒå™¨")
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"ğŸ“ æ•°æ®æ ¹ç›®å½•: {data_root}")
        print(f"ğŸ¯ æ‰¹æ¬¡å¤§å°: {self.config['batch_size']}")
        print(f"ğŸ¯ å·¥ä½œçº¿ç¨‹: {self.config['num_workers']}")
        print(f"ğŸ¯ æ··åˆç²¾åº¦: {self.config['mixed_precision']}")
        print("ğŸ¯ é›†æˆäº†ä¼˜åŒ–çš„äººä½“é€‰æ‹©ç®—æ³•")
        
        # ğŸ”§ æ··åˆç²¾åº¦è®­ç»ƒ
        if self.config['mixed_precision']:
            self.scaler = amp.GradScaler()
            print("âš¡ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒåŠ é€Ÿ")
        
        # éªŒè¯æ•°æ®ç›®å½•
        self._validate_data_directories()
    
    def _validate_data_directories(self):
        """éªŒè¯æ•°æ®ç›®å½•ç»“æ„"""
        required_dirs = ['train', 'val', 'test']
        for dir_name in required_dirs:
            dir_path = os.path.join(self.data_root, dir_name)
            if not os.path.exists(dir_path):
                raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°å¿…éœ€çš„ç›®å½•: {dir_path}")
        print("âœ… æ•°æ®ç›®å½•ç»“æ„éªŒè¯é€šè¿‡")
    
    def train(self, epochs: int = TRAINING_CONFIG['max_epochs'], 
              save_path: str = "badminton_model.pth"):
        """
        è®­ç»ƒæ¨¡å‹ - GPUä¼˜åŒ–ç‰ˆæœ¬
        
        Args:
            epochs: è®­ç»ƒè½®æ•°
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        """
        print("=" * 60)
        print("ğŸ¸ å¼€å§‹è®­ç»ƒç¾½æ¯›çƒåŠ¨ä½œåˆ†ç±»æ¨¡å‹ (GPUä¼˜åŒ–)")
        print("=" * 60)
        
        training_start_time = time.time()
        
        # åŠ è½½æ•°æ®é›†
        print("ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®é›†...")
        use_gpu_models = self.device.type == 'cuda'
        
        train_dataset = VideoBadmintonDataset(f"{self.data_root}/train/", use_gpu=use_gpu_models)
        val_dataset = VideoBadmintonDataset(f"{self.data_root}/val/", use_gpu=use_gpu_models)
        test_dataset = VideoBadmintonDataset(f"{self.data_root}/test/", use_gpu=use_gpu_models)
        
        print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ")
        print(f"   è®­ç»ƒé›†: {len(train_dataset)} ä¸ªæ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(val_dataset)} ä¸ªæ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(test_dataset)} ä¸ªæ ·æœ¬")
        
        # ğŸ”§ GPUä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=self.config['num_workers'],
            pin_memory=self.config['pin_memory'],
            persistent_workers=self.config['persistent_workers'],
            prefetch_factor=self.config['prefetch_factor'],
            drop_last=True  # ç¡®ä¿æ‰¹æ¬¡å¤§å°ä¸€è‡´ï¼Œæœ‰åˆ©äºBatchNorm
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=self.config['num_workers'],
            pin_memory=self.config['pin_memory'],
            persistent_workers=self.config['persistent_workers'],
            prefetch_factor=self.config['prefetch_factor']
        )
        
        # åˆå§‹åŒ–æ¨¡å‹
        detector = BadmintonDetector()
        model = detector.action_classifier.to(self.device)
        
        # ğŸ”§ GPUä¼˜åŒ–ï¼šæ¨¡å‹ç¼–è¯‘ï¼ˆPyTorch 2.0+ï¼‰
        if torch.cuda.is_available() and hasattr(torch, 'compile'):
            try:
                model = torch.compile(model, mode='max-autotune')
                print("âš¡ æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ æ¨¡å‹ç¼–è¯‘å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å¼: {e}")
        
        # è®­ç»ƒé…ç½®
        criterion = torch.nn.CrossEntropyLoss()
        
        # ğŸ”§ GPUä¼˜åŒ–ï¼šä½¿ç”¨AdamWä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡scaling
        base_lr = MODEL_CONFIG['learning_rate']
        scaled_lr = base_lr * (self.config['batch_size'] / 16)  # æ ¹æ®æ‰¹æ¬¡å¤§å°è°ƒæ•´å­¦ä¹ ç‡
        
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=scaled_lr,
            weight_decay=MODEL_CONFIG['weight_decay'],
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            patience=TRAINING_CONFIG['lr_scheduler_patience'],
            factor=0.5,
            verbose=True
        )
        
        # è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
        best_val_accuracy = 0.0
        epochs_without_improvement = 0
        training_history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'learning_rate': []
        }
        
        print(f"ğŸ”§ GPUè®­ç»ƒé…ç½®:")
        print(f"   æ‰¹æ¬¡å¤§å°: {self.config['batch_size']}")
        print(f"   åŸºç¡€å­¦ä¹ ç‡: {base_lr:.6f}")
        print(f"   ç¼©æ”¾å­¦ä¹ ç‡: {scaled_lr:.6f}")
        print(f"   æœ€å¤§è½®æ•°: {epochs}")
        print("-" * 40)
        
        # ä¸»è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            epoch_start_time = time.time()
            
            print(f"\nğŸ“… Epoch {epoch+1}/{epochs}")
            print("-" * 30)
            
            # è®­ç»ƒé˜¶æ®µ
            if self.config['mixed_precision']:
                train_loss, train_acc = self._train_epoch_amp(model, train_loader, criterion, optimizer)
            else:
                train_loss, train_acc = self._train_epoch(model, train_loader, criterion, optimizer)
            
            # éªŒè¯é˜¶æ®µ
            val_loss, val_acc = self._validate_epoch(model, val_loader, criterion)
            
            # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
            scheduler.step(val_loss)
            current_lr = optimizer.param_groups[0]['lr']
            
            # è®°å½•è®­ç»ƒå†å²
            training_history['train_loss'].append(train_loss)
            training_history['train_acc'].append(train_acc)
            training_history['val_loss'].append(val_loss)
            training_history['val_acc'].append(val_acc)
            training_history['learning_rate'].append(current_lr)
            
            # æ˜¾ç¤ºå½“å‰è½®ç»“æœ
            epoch_time = time.time() - epoch_start_time
            
            # ğŸ”§ GPUå†…å­˜ç›‘æ§
            if torch.cuda.is_available():
                gpu_memory_used = torch.cuda.max_memory_allocated() / 1024**3
                gpu_memory_cached = torch.cuda.max_memory_reserved() / 1024**3
                gpu_info = f"| GPUå†…å­˜: {gpu_memory_used:.1f}GB/{gpu_memory_cached:.1f}GB"
            else:
                gpu_info = ""
            
            print(f"ğŸ“Š è®­ç»ƒæŸå¤±: {train_loss:.4f} | è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}%")
            print(f"ğŸ“Š éªŒè¯æŸå¤±: {val_loss:.4f} | éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
            print(f"â±ï¸  è½®æ¬¡è€—æ—¶: {epoch_time:.1f}ç§’ {gpu_info} | å­¦ä¹ ç‡: {current_lr:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_accuracy:
                best_val_accuracy = val_acc
                epochs_without_improvement = 0
                
                torch.save(model.state_dict(), save_path)
                print(f"ğŸ¯ æ–°çš„æœ€ä½³æ¨¡å‹ï¼éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}% (å·²ä¿å­˜)")
            else:
                epochs_without_improvement += 1
            
            # æ—©åœæ£€æŸ¥
            if epochs_without_improvement >= TRAINING_CONFIG['early_stopping_patience']:
                print(f"â¹ï¸  æ—©åœè§¦å‘ï¼{epochs_without_improvement} è½®æ— æ”¹è¿›")
                break
            
            # ğŸ”§ æ¸…ç†GPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        # è®­ç»ƒç»“æŸç»Ÿè®¡
        total_training_time = time.time() - training_start_time
        print("\n" + "=" * 60)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_training_time:.1f}ç§’")
        print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_accuracy:.2f}%")
        print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = save_path.replace('.pth', '_history.json')
        with open(history_path, 'w') as f:
            json.dump(training_history, f, indent=2)
        print(f"ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜è‡³: {history_path}")
        
        # æœ€ç»ˆæµ‹è¯•è¯„ä¼°
        if os.path.exists(save_path):
            self._final_evaluation(test_dataset, save_path)
    
    def _train_epoch_amp(self, model, train_loader, criterion, optimizer) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch - æ··åˆç²¾åº¦ç‰ˆæœ¬"""
        model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
            
            optimizer.zero_grad(set_to_none=True)  # æ›´é«˜æ•ˆçš„æ¢¯åº¦æ¸…é›¶
            
            # ğŸ”§ æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            with amp.autocast():
                output = model(data)
                loss = criterion(output, target)
            
            # ğŸ”§ æ··åˆç²¾åº¦åå‘ä¼ æ’­
            self.scaler.scale(loss).backward()
            self.scaler.step(optimizer)
            self.scaler.update()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            # æ˜¾ç¤ºè¿›åº¦
            if batch_idx % 50 == 0:  # GPUè®­ç»ƒæ›´å¿«ï¼Œå‡å°‘è¾“å‡ºé¢‘ç‡
                current_acc = 100.0 * correct / total
                print(f"  ğŸ“¦ æ‰¹æ¬¡ {batch_idx}/{len(train_loader)} | "
                      f"æŸå¤±: {loss.item():.4f} | å‡†ç¡®ç‡: {current_acc:.2f}%")
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100.0 * correct / total
        return avg_loss, accuracy
    
    def _train_epoch(self, model, train_loader, criterion, optimizer) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch - æ ‡å‡†ç‰ˆæœ¬"""
        model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
            
            optimizer.zero_grad(set_to_none=True)
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            
            # ğŸ”§ æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            # æ˜¾ç¤ºè¿›åº¦
            if batch_idx % 50 == 0:
                current_acc = 100.0 * correct / total
                print(f"  ğŸ“¦ æ‰¹æ¬¡ {batch_idx}/{len(train_loader)} | "
                      f"æŸå¤±: {loss.item():.4f} | å‡†ç¡®ç‡: {current_acc:.2f}%")
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100.0 * correct / total
        return avg_loss, accuracy
    
    def _validate_epoch(self, model, val_loader, criterion) -> Tuple[float, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
                
                if self.config['mixed_precision']:
                    with amp.autocast():
                        output = model(data)
                        loss = criterion(output, target)
                else:
                    output = model(data)
                    loss = criterion(output, target)
                
                total_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()
        
        avg_loss = total_loss / len(val_loader)
        accuracy = 100.0 * correct / total
        return avg_loss, accuracy
    
    def _final_evaluation(self, test_dataset, model_path):
        """æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°"""
        print("\n" + "=" * 60)
        print("ğŸ§ª æœ€ç»ˆè¯„ä¼° (ä½¿ç”¨æµ‹è¯•é›†)")
        print("=" * 60)
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=self.config['num_workers'],
            pin_memory=self.config['pin_memory']
        )
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        detector = BadmintonDetector()
        model = detector.action_classifier.to(self.device)
        model.load_state_dict(torch.load(model_path, map_location=self.device))
        model.eval()
        
        # è¯„ä¼°
        correct = 0
        total = 0
        class_correct = [0] * 18
        class_total = [0] * 18
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
                
                if self.config['mixed_precision']:
                    with amp.autocast():
                        output = model(data)
                else:
                    output = model(data)
                
                _, predicted = torch.max(output, 1)
                
                total += target.size(0)
                correct += (predicted == target).sum().item()
                
                # æŒ‰ç±»åˆ«ç»Ÿè®¡
                for i in range(target.size(0)):
                    label = target[i].item()
                    class_total[label] += 1
                    if predicted[i] == target[i]:
                        class_correct[label] += 1
        
        # è¾“å‡ºæ•´ä½“ç»“æœ
        overall_accuracy = 100.0 * correct / total
        print(f"ğŸ¯ æµ‹è¯•é›†æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy:.2f}% ({correct}/{total})")
        
        # è¾“å‡ºå„ç±»åˆ«è¯¦ç»†ç»“æœ
        print("\nğŸ“Š å„ç±»åˆ«è¯¦ç»†ç»“æœ:")
        print("-" * 80)
        print(f"{'ç±»åˆ«ID':<6} {'åŠ¨ä½œåç§°':<20} {'å‡†ç¡®ç‡':<10} {'æ ·æœ¬æ•°':<10}")
        print("-" * 80)
        
        for i in range(18):
            if class_total[i] > 0:
                acc = 100.0 * class_correct[i] / class_total[i]
                class_name = RAW_CLASSES.get(i, f"Class_{i}")
                print(f"{i:02d}     {class_name:<20} {acc:>6.2f}%     {class_total[i]:>4d}")
        
        print("-" * 80)
        print("âœ… æœ€ç»ˆè¯„ä¼°å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è®­ç»ƒç¾½æ¯›çƒåŠ¨ä½œåˆ†ç±»æ¨¡å‹ (GPUä¼˜åŒ–)")
    parser.add_argument("--data", default="data/split/", 
                       help="åˆ†å‰²åçš„æ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument("--epochs", type=int, default=TRAINING_CONFIG['max_epochs'], 
                       help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--output", default="badminton_model_gpu.pth", 
                       help="æ¨¡å‹è¾“å‡ºè·¯å¾„")
    parser.add_argument("--batch-size", type=int, default=None, 
                       help="æ‰¹æ¬¡å¤§å°ï¼ˆç•™ç©ºè‡ªåŠ¨ä¼˜åŒ–ï¼‰")
    parser.add_argument("--lr", type=float, default=MODEL_CONFIG['learning_rate'], 
                       help="å­¦ä¹ ç‡")
    parser.add_argument("--cpu", action="store_true", 
                       help="å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼")
    
    args = parser.parse_args()
    
    # æ›´æ–°é…ç½®
    if args.batch_size:
        MODEL_CONFIG['batch_size'] = args.batch_size
    MODEL_CONFIG['learning_rate'] = args.lr
    
    # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    
    # ğŸ”§ GPUä¼˜åŒ–è®¾ç½®
    if torch.cuda.is_available() and not args.cpu:
        torch.cuda.manual_seed(42)
        torch.cuda.manual_seed_all(42)
        torch.backends.cudnn.deterministic = False  # å…è®¸éç¡®å®šæ€§ä»¥æé«˜æ€§èƒ½
        torch.backends.cudnn.benchmark = True       # ä¼˜åŒ–å·ç§¯æ€§èƒ½
    
    # å¼€å§‹è®­ç»ƒ
    trainer = Trainer(args.data, force_cpu=args.cpu)
    trainer.train(epochs=args.epochs, save_path=args.output)

if __name__ == "__main__":
    main()
